{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SignXAI2 with TensorFlow - Basic Usage (VGG16)\n\nThis tutorial demonstrates how to use the SignXAI2 package with TensorFlow to explain a VGG16 image classification model. We'll walk through:\n\n1. Setting up the environment\n2. Loading a pre-trained VGG16 model and sample image\n3. Generating explanations using the unified API\n4. Visualizing and comparing the results\n\n## Setup Requirements\n\n**Important**: SignXAI2 requires Python 3.9 or 3.10 (Python 3.11+ is not supported)\n\nSince you're running this tutorial, you should already have cloned the signxai2 repository. From the repository root directory:\n\n### Using conda:\n```bash\n# Create environment with Python 3.10\nconda create -n signxai2 python=3.10\nconda activate signxai2\n\n# Install SignXAI2 with TensorFlow support\npip install signxai2[tensorflow]\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\n### Using venv:\n```bash\n# Create virtual environment\npython3.10 -m venv signxai2_env\nsource signxai2_env/bin/activate  # On Windows: signxai2_env\\Scripts\\activate\n\n# Install SignXAI2 with TensorFlow support\npip install signxai2[tensorflow]\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\nLet's get started!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": "import os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n\n# SignXAI2 unified API imports\nfrom signxai import explain, list_methods\nfrom signxai.utils.utils import load_image",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and model paths\n",
    "_THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "DATA_DIR = os.path.realpath(os.path.join(_THIS_DIR, \"..\", \"..\", \"data\"))\n",
    "VGG16_MODEL_PATH = os.path.join(DATA_DIR, \"models\", \"tensorflow\", \"VGG16\", \"model.h5\")\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, \"images\", \"example.jpg\")\n",
    "\n",
    "# Check that files exist\n",
    "assert os.path.exists(VGG16_MODEL_PATH), f\"VGG16 model not found at {VGG16_MODEL_PATH}\"\n",
    "assert os.path.exists(IMAGE_PATH), f\"Image not found at {IMAGE_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load VGG16 Model and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the VGG16 model\ntry:\n    # First try loading our pre-saved model\n    vgg16_model = tf.keras.models.load_model(VGG16_MODEL_PATH)\n    print(\"Loaded pre-saved VGG16 model\")\nexcept Exception as e:\n    # If that fails, load the model from Keras applications\n    print(f\"Could not load saved model: {e}\\nLoading from Keras applications instead\")\n    vgg16_model = tf.keras.applications.VGG16(weights='imagenet', include_top=True)\n    \nvgg16_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load and preprocess image specifically for VGG16\noriginal_img, preprocessed_img = load_image(IMAGE_PATH, target_size=(224, 224), expand_dims=True)\n\n# Display the original image\nplt.figure(figsize=(6, 6))\nplt.imshow(original_img)\nplt.axis('off')\nplt.title('Sample Image')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict the Class with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with VGG16\n",
    "predictions = vgg16_model.predict(preprocessed_img)\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "\n",
    "# Decode the prediction (VGG16 is trained on ImageNet)\n",
    "try:\n",
    "    decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
    "        print(f\"{i+1}: {label} ({score:.4f})\")\n",
    "    \n",
    "    # Set the class label\n",
    "    class_name = decoded_predictions[0][1]\n",
    "except:\n",
    "    class_name = f\"Class {predicted_class}\"\n",
    "    print(f\"Predicted class index: {predicted_class}\")\n",
    "\n",
    "print(f\"\\nExplaining prediction: {class_name} (class index: {predicted_class})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Explanations with SignXAI\n",
    "\n",
    "Now let's use SignXAI to explain the VGG16 model's prediction using different methods."
   ]
  },
  {
   "cell_type": "code",
   "source": "# List all available methods in SignXAI2\nprint(\"Available methods in SignXAI2:\")\navailable_methods = list_methods()\nprint(f\"Total methods: {len(available_methods)}\")\nprint(\"\\nSome common methods:\")\nfor method in ['gradient', 'gradient_x_input', 'grad_cam', 'guided_backprop', 'lrp_z']:\n    if method in available_methods:\n        print(f\"  - {method}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the methods we want to use for VGG16 explanation\n# Using the unified API method names\nmethods_to_test = [\n    'gradient',\n    'gradient_x_input',\n    'grad_cam',\n    'guided_backprop', \n    'lrp_z',\n    'lrp_epsilon_0_1'\n]\n\n# Additional parameters for specific methods\nmethod_params = {\n    'grad_cam': {'layer_name': 'block5_conv3'}  # VGG16-specific last conv layer\n}\n\n# Storage for explanations\nexplanations = {}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate explanations using the unified API\nfor method_name in methods_to_test:\n    print(f\"Generating {method_name} explanation...\")\n    \n    # Get method-specific parameters\n    params = method_params.get(method_name, {})\n    \n    # Use the unified explain API\n    explanation = explain(\n        model=vgg16_model,\n        x=preprocessed_img,\n        method_name=method_name,\n        target_class=predicted_class,\n        **params\n    )\n    \n    explanations[method_name] = explanation\n    \nprint(\"All explanations generated!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize VGG16 Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Utility function for prettier visualization\ndef preprocess_explanation(explanation):\n    # Remove batch dimension if present\n    if explanation.ndim == 4:\n        explanation = explanation[0]\n    \n    # Convert to RGB if single channel\n    if explanation.ndim == 2:\n        explanation = np.expand_dims(explanation, axis=-1)\n        explanation = np.repeat(explanation, 3, axis=-1)\n    elif explanation.shape[-1] == 1:\n        explanation = np.repeat(explanation, 3, axis=-1)\n    \n    # Use absolute values for visualization\n    abs_explanation = np.abs(explanation)\n    \n    # Normalize for visualization\n    if abs_explanation.max() > 0:\n        abs_explanation = abs_explanation / abs_explanation.max()\n    \n    return abs_explanation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create figure for visualization\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, (method_name, explanation) in enumerate(explanations.items()):\n    processed_explanation = preprocess_explanation(explanation)\n    \n    # Simple overlay visualization\n    axes[i].imshow(original_img)\n    axes[i].imshow(processed_explanation, alpha=0.5, cmap='hot')\n    axes[i].set_title(method_name.replace('_', ' ').title())\n    axes[i].axis('off')\n\nplt.suptitle(f\"VGG16 Explanations for class: {class_name}\", fontsize=16)\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)  # Adjust for the suptitle\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpret the VGG16 Results\n",
    "\n",
    "Let's interpret what we're seeing in these explanation methods for the VGG16 model:\n",
    "\n",
    "- **Gradient**: Shows pixel-level importance via the gradient of the output with respect to input. In VGG16, this often highlights edges and textures.\n",
    "\n",
    "- **Gradient Ã— Input**: Enhances gradient by multiplication with input values. This tends to focus more on the regions where both the gradient and input values are high.\n",
    "\n",
    "- **GradCAM**: Uses the last convolutional layer of VGG16 (block5_conv3) to produce a coarse localization map highlighting important regions for the prediction.\n",
    "\n",
    "- **Guided Backprop**: Creates sharper feature visualizations by modifying the backpropagation signal through ReLU layers. It's particularly effective for VGG16 which has many ReLU activations.\n",
    "\n",
    "- **LRP-Z**: Layer-wise Relevance Propagation with the Z-rule propagates the prediction backward through the network to identify relevant input features.\n",
    "\n",
    "- **LRP-Epsilon**: A variant of LRP that adds a stabilizing term (epsilon) to avoid division by zero, producing slightly different attribution maps.\n",
    "\n",
    "Each method highlights different aspects of how VGG16 processes the image to make its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this tutorial, we've learned how to:\n",
    "- Set up SignXAI with TensorFlow\n",
    "- Load and prepare a pre-trained VGG16 model for explanation\n",
    "- Apply various explainability methods to understand VGG16 predictions\n",
    "- Visualize and interpret the results\n",
    "\n",
    "VGG16 is an excellent model for explainability demonstrations because of its straightforward architecture. The clear convolutional structure makes it easier to interpret how different parts of the image influence the model's predictions.\n",
    "\n",
    "For more advanced techniques and detailed explanations of other models, check out the other tutorials in this series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}