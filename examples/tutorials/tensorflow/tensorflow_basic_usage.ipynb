{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SignXAI with TensorFlow - Basic Usage (VGG16)\n\nThis tutorial demonstrates how to use the SignXAI package with TensorFlow to explain a VGG16 image classification model. We'll walk through:\n\n1. Setting up the environment\n2. Loading a pre-trained VGG16 model and sample image\n3. Generating explanations using different methods\n4. Visualizing and comparing the results\n\n## Setup Requirements\n\nFor this TensorFlow tutorial, you'll need to install SignXAI with TensorFlow dependencies:\n\n```bash\n# For conda users\nconda create -n signxai-tensorflow python=3.8\nconda activate signxai-tensorflow\npip install -r ../../requirements/common.txt\npip install -r ../../requirements/tensorflow.txt\n\n# Or for pip users\npython -m venv signxai_tensorflow_env\nsource signxai_tensorflow_env/bin/activate  # On Windows: signxai_tensorflow_env\\Scripts\\activate\npip install -r ../../requirements/common.txt\npip install -r ../../requirements/tensorflow.txt\n```\n\nLet's get started!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "# SignXAI imports\n",
    "from signxai.tf_signxai.methods import (\n",
    "    SIGN,\n",
    "    GradCAM,\n",
    "    GuidedBackprop,\n",
    "    LRPZ,\n",
    "    LRPEpsilon\n",
    ")\n",
    "from signxai.common.visualization import visualize_attribution\n",
    "from signxai.utils.utils import remove_softmax, load_image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and model paths\n",
    "_THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "DATA_DIR = os.path.realpath(os.path.join(_THIS_DIR, \"..\", \"..\", \"data\"))\n",
    "VGG16_MODEL_PATH = os.path.join(DATA_DIR, \"models\", \"tensorflow\", \"VGG16\", \"model.h5\")\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, \"images\", \"example.jpg\")\n",
    "\n",
    "# Check that files exist\n",
    "assert os.path.exists(VGG16_MODEL_PATH), f\"VGG16 model not found at {VGG16_MODEL_PATH}\"\n",
    "assert os.path.exists(IMAGE_PATH), f\"Image not found at {IMAGE_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load VGG16 Model and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model\n",
    "try:\n",
    "    # First try loading our pre-saved model\n",
    "    vgg16_model = tf.keras.models.load_model(VGG16_MODEL_PATH)\n",
    "    print(\"Loaded pre-saved VGG16 model\")\n",
    "except Exception as e:\n",
    "    # If that fails, load the model from Keras applications\n",
    "    print(f\"Could not load saved model: {e}\\nLoading from Keras applications instead\")\n",
    "    vgg16_model = tf.keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "    \n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove softmax from the VGG16 model - crucial step for many explainability methods\n",
    "vgg16_model_no_softmax = remove_softmax(vgg16_model)\n",
    "\n",
    "# Load and preprocess image specifically for VGG16\n",
    "original_img, preprocessed_img = load_image(IMAGE_PATH, target_size=(224, 224), expand_dims=True)\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(original_img)\n",
    "plt.axis('off')\n",
    "plt.title('Sample Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict the Class with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with VGG16\n",
    "predictions = vgg16_model.predict(preprocessed_img)\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "\n",
    "# Decode the prediction (VGG16 is trained on ImageNet)\n",
    "try:\n",
    "    decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
    "        print(f\"{i+1}: {label} ({score:.4f})\")\n",
    "    \n",
    "    # Set the class label\n",
    "    class_name = decoded_predictions[0][1]\n",
    "except:\n",
    "    class_name = f\"Class {predicted_class}\"\n",
    "    print(f\"Predicted class index: {predicted_class}\")\n",
    "\n",
    "print(f\"\\nExplaining prediction: {class_name} (class index: {predicted_class})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Explanations with SignXAI\n",
    "\n",
    "Now let's use SignXAI to explain the VGG16 model's prediction using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the methods we want to use for VGG16 explanation\n",
    "methods = {\n",
    "    'Gradient': SIGN(vgg16_model_no_softmax),\n",
    "    'Gradient × Input': SIGN(vgg16_model_no_softmax),  # We'll multiply by input later\n",
    "    'GradCAM': GradCAM(vgg16_model_no_softmax, last_conv_layer_name='block5_conv3'),  # VGG16-specific layer\n",
    "    'Guided Backprop': GuidedBackprop(vgg16_model_no_softmax),\n",
    "    'LRP-Z': LRPZ(vgg16_model_no_softmax),\n",
    "    'LRP-Epsilon': LRPEpsilon(vgg16_model_no_softmax, epsilon=0.1)\n",
    "}\n",
    "\n",
    "# Storage for explanations\n",
    "explanations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanations for VGG16\n",
    "for method_name, explainer in methods.items():\n",
    "    print(f\"Generating {method_name} explanation...\")\n",
    "    \n",
    "    if method_name == 'Gradient × Input':\n",
    "        # Special case for gradient × input\n",
    "        grad = explainer.attribute(preprocessed_img, target_class=predicted_class).numpy()\n",
    "        explanation = grad * preprocessed_img[0].numpy()\n",
    "    else:\n",
    "        explanation = explainer.attribute(preprocessed_img, target_class=predicted_class).numpy()\n",
    "    \n",
    "    explanations[method_name] = explanation\n",
    "    \n",
    "print(\"All explanations generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize VGG16 Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for prettier visualization\n",
    "def preprocess_explanation(explanation):\n",
    "    # Remove batch dimension if present\n",
    "    if explanation.ndim == 4:\n",
    "        explanation = explanation[0]\n",
    "    \n",
    "    # Use absolute values for gradient-based methods\n",
    "    abs_explanation = np.abs(explanation)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    if abs_explanation.max() > 0:\n",
    "        abs_explanation = abs_explanation / abs_explanation.max()\n",
    "    \n",
    "    return abs_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure for visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (method_name, explanation) in enumerate(explanations.items()):\n",
    "    processed_explanation = preprocess_explanation(explanation)\n",
    "    \n",
    "    # Use SignXAI's visualization utility\n",
    "    visualize_attribution(image=original_img, attribution=processed_explanation, ax=axes[i])\n",
    "    axes[i].set_title(method_name)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"VGG16 Explanations for class: {class_name}\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)  # Adjust for the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpret the VGG16 Results\n",
    "\n",
    "Let's interpret what we're seeing in these explanation methods for the VGG16 model:\n",
    "\n",
    "- **Gradient**: Shows pixel-level importance via the gradient of the output with respect to input. In VGG16, this often highlights edges and textures.\n",
    "\n",
    "- **Gradient × Input**: Enhances gradient by multiplication with input values. This tends to focus more on the regions where both the gradient and input values are high.\n",
    "\n",
    "- **GradCAM**: Uses the last convolutional layer of VGG16 (block5_conv3) to produce a coarse localization map highlighting important regions for the prediction.\n",
    "\n",
    "- **Guided Backprop**: Creates sharper feature visualizations by modifying the backpropagation signal through ReLU layers. It's particularly effective for VGG16 which has many ReLU activations.\n",
    "\n",
    "- **LRP-Z**: Layer-wise Relevance Propagation with the Z-rule propagates the prediction backward through the network to identify relevant input features.\n",
    "\n",
    "- **LRP-Epsilon**: A variant of LRP that adds a stabilizing term (epsilon) to avoid division by zero, producing slightly different attribution maps.\n",
    "\n",
    "Each method highlights different aspects of how VGG16 processes the image to make its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this tutorial, we've learned how to:\n",
    "- Set up SignXAI with TensorFlow\n",
    "- Load and prepare a pre-trained VGG16 model for explanation\n",
    "- Apply various explainability methods to understand VGG16 predictions\n",
    "- Visualize and interpret the results\n",
    "\n",
    "VGG16 is an excellent model for explainability demonstrations because of its straightforward architecture. The clear convolutional structure makes it easier to interpret how different parts of the image influence the model's predictions.\n",
    "\n",
    "For more advanced techniques and detailed explanations of other models, check out the other tutorials in this series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
