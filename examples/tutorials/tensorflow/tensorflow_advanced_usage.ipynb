{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SignXAI2 with TensorFlow - Advanced Usage\n\nThis tutorial demonstrates advanced techniques for using SignXAI2 with TensorFlow models. It builds on the basic tutorial and explores more sophisticated explainability methods and customizations.\n\n## Setup Requirements\n\n**Important**: SignXAI2 requires Python 3.9 or 3.10 (Python 3.11+ is not supported)\n\nSince you're running this tutorial, you should already have cloned the signxai2 repository. From the repository root directory:\n\n### Using conda:\n```bash\n# Create environment with Python 3.10\nconda create -n signxai2 python=3.10\nconda activate signxai2\n\n# Install SignXAI2 with TensorFlow support\npip install signxai2[tensorflow]\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\n### Using venv:\n```bash\n# Create virtual environment\npython3.10 -m venv signxai2_env\nsource signxai2_env/bin/activate  # On Windows: signxai2_env\\Scripts\\activate\n\n# Install SignXAI2 with TensorFlow support\npip install signxai2[tensorflow]\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\n## Overview\n\nIn this tutorial, we'll cover:\n1. Using different LRP variants with the unified API\n2. Advanced method parameters and configurations\n3. Working with custom model architectures\n4. Combining multiple explanation methods\n5. Advanced visualization techniques"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n\n# SignXAI2 unified API imports\nfrom signxai import explain, list_methods, get_method_info\nfrom signxai.utils.utils import load_image"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up data and model paths\n_THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\nDATA_DIR = os.path.realpath(os.path.join(_THIS_DIR, \"..\", \"..\", \"data\"))\nVGG16_MODEL_PATH = os.path.join(DATA_DIR, \"models\", \"tensorflow\", \"VGG16\", \"model.h5\")\nIMAGE_PATH = os.path.join(DATA_DIR, \"images\", \"example.jpg\")\n\n# Verify paths\nassert os.path.exists(VGG16_MODEL_PATH), f\"VGG16 model not found at {VGG16_MODEL_PATH}\"\nassert os.path.exists(IMAGE_PATH), f\"Image not found at {IMAGE_PATH}\"\n\n# Load model\ntry:\n    model = tf.keras.models.load_model(VGG16_MODEL_PATH)\n    print(\"Loaded pre-saved VGG16 model\")\nexcept Exception as e:\n    print(f\"Could not load saved model: {e}\\nLoading from Keras applications instead\")\n    model = tf.keras.applications.VGG16(weights='imagenet', include_top=True)\n\n# Load and display image\noriginal_img, preprocessed_img = load_image(IMAGE_PATH, target_size=(224, 224), expand_dims=True)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(original_img)\nplt.axis('off')\nplt.title('Sample Image')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Using Different LRP Variants with the Unified API\n\nSignXAI2 provides various LRP (Layer-wise Relevance Propagation) methods through the unified API. Let's explore different LRP rules and configurations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get a prediction for reference\npredictions = model.predict(preprocessed_img)\npredicted_class = np.argmax(predictions[0])\n\n# Decode the prediction\ntry:\n    decoded_predictions = decode_predictions(predictions, top=3)[0]\n    print(\"Top 3 predictions:\")\n    for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n        print(f\"{i+1}: {label} ({score:.4f})\")\n    class_name = decoded_predictions[0][1]\nexcept:\n    class_name = f\"Class {predicted_class}\"\n    \nprint(f\"\\nExplaining prediction: {class_name} (class index: {predicted_class})\")\n\n# Different LRP variants available in SignXAI2\nlrp_methods = [\n    'lrp_z',\n    'lrp_epsilon_0_001',\n    'lrp_epsilon_0_01', \n    'lrp_epsilon_0_1',\n    'lrp_epsilon_0_2',\n    'lrp_alpha_1_beta_0',\n    'lrp_alpha_2_beta_1',\n    'lrpsign_z',\n    'lrpsign_epsilon_0_1'\n]\n\n# Generate explanations using different LRP variants\nlrp_explanations = {}\nfor method in lrp_methods:\n    if method in list_methods():\n        print(f\"Generating {method} explanation...\")\n        explanation = explain(\n            model=model,\n            x=preprocessed_img,\n            method_name=method,\n            target_class=predicted_class\n        )\n        lrp_explanations[method] = explanation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for visualization\n",
    "def preprocess_explanation(explanation):\n",
    "    # Use absolute values for attribution\n",
    "    abs_explanation = np.abs(explanation)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    if abs_explanation.max() > 0:\n",
    "        abs_explanation = abs_explanation / abs_explanation.max()\n",
    "    \n",
    "    return abs_explanation\n",
    "\n",
    "# Visualize all LRP variants\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get numpy array of original image for visualization\n",
    "np_img = np.array(original_img)\n",
    "\n",
    "# Plot the original image in the first position\n",
    "axes[0].imshow(np_img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot all LRP variants in the remaining positions\n",
    "for i, (method_name, explanation) in enumerate(lrp_explanations.items(), 1):\n",
    "    if i < len(axes):\n",
    "        visualize_attribution(np_img, preprocess_explanation(explanation), ax=axes[i])\n",
    "        axes[i].set_title(method_name)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Utility function for visualization\ndef preprocess_explanation(explanation):\n    # Remove batch dimension if present\n    if explanation.ndim == 4:\n        explanation = explanation[0]\n    \n    # Convert to RGB if single channel\n    if explanation.ndim == 2:\n        explanation = np.expand_dims(explanation, axis=-1)\n        explanation = np.repeat(explanation, 3, axis=-1)\n    elif explanation.shape[-1] == 1:\n        explanation = np.repeat(explanation, 3, axis=-1)\n    \n    # Use absolute values for attribution\n    abs_explanation = np.abs(explanation)\n    \n    # Normalize for visualization\n    if abs_explanation.max() > 0:\n        abs_explanation = abs_explanation / abs_explanation.max()\n    \n    return abs_explanation\n\n# Visualize all LRP variants\nn_methods = len(lrp_explanations)\nn_cols = 3\nn_rows = (n_methods + 1) // n_cols + 1  # +1 for original image\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6*n_rows))\naxes = axes.flatten()\n\n# Plot the original image in the first position\naxes[0].imshow(original_img)\naxes[0].set_title(\"Original Image\")\naxes[0].axis('off')\n\n# Plot all LRP variants\nfor i, (method_name, explanation) in enumerate(lrp_explanations.items(), 1):\n    if i < len(axes):\n        processed_explanation = preprocess_explanation(explanation)\n        axes[i].imshow(original_img)\n        axes[i].imshow(processed_explanation, alpha=0.5, cmap='hot')\n        axes[i].set_title(method_name.replace('_', ' ').upper())\n        axes[i].axis('off')\n\n# Hide unused subplots\nfor i in range(len(lrp_explanations) + 1, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. Advanced Method Parameters and Configurations\n\nSignXAI2's unified API allows you to customize explanation methods with various parameters. Let's explore some advanced configurations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: SmoothGrad with different noise levels\nsmoothgrad_params = [\n    {'noise_scale': 0.05, 'augment_by_n': 10},\n    {'noise_scale': 0.1, 'augment_by_n': 25},\n    {'noise_scale': 0.2, 'augment_by_n': 50}\n]\n\nsmoothgrad_explanations = {}\nfor params in smoothgrad_params:\n    label = f\"SmoothGrad (noise={params['noise_scale']}, n={params['augment_by_n']})\"\n    print(f\"Generating {label}...\")\n    explanation = explain(\n        model=model,\n        x=preprocessed_img,\n        method_name='smoothgrad',\n        target_class=predicted_class,\n        **params\n    )\n    smoothgrad_explanations[label] = explanation\n\n# Example 2: Integrated Gradients with different step counts\nig_steps = [10, 25, 50]\nig_explanations = {}\nfor steps in ig_steps:\n    label = f\"Integrated Gradients ({steps} steps)\"\n    print(f\"Generating {label}...\")\n    explanation = explain(\n        model=model,\n        x=preprocessed_img,\n        method_name='integrated_gradients',\n        target_class=predicted_class,\n        steps=steps\n    )\n    ig_explanations[label] = explanation\n\n# Example 3: Grad-CAM with different layers\ngradcam_layers = ['block4_conv3', 'block5_conv3']  # Different VGG16 layers\ngradcam_explanations = {}\nfor layer in gradcam_layers:\n    label = f\"Grad-CAM ({layer})\"\n    print(f\"Generating {label}...\")\n    explanation = explain(\n        model=model,\n        x=preprocessed_img,\n        method_name='grad_cam',\n        target_class=predicted_class,\n        layer_name=layer\n    )\n    gradcam_explanations[label] = explanation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Visualize parameter variations\nfig, axes = plt.subplots(3, 3, figsize=(18, 18))\n\n# Row 1: SmoothGrad variations\nfor i, (label, explanation) in enumerate(smoothgrad_explanations.items()):\n    processed = preprocess_explanation(explanation)\n    axes[0, i].imshow(original_img)\n    axes[0, i].imshow(processed, alpha=0.5, cmap='hot')\n    axes[0, i].set_title(label)\n    axes[0, i].axis('off')\n\n# Row 2: Integrated Gradients variations\nfor i, (label, explanation) in enumerate(ig_explanations.items()):\n    processed = preprocess_explanation(explanation)\n    axes[1, i].imshow(original_img)\n    axes[1, i].imshow(processed, alpha=0.5, cmap='hot')\n    axes[1, i].set_title(label)\n    axes[1, i].axis('off')\n\n# Row 3: Grad-CAM variations\nfor i, (label, explanation) in enumerate(gradcam_explanations.items()):\n    processed = preprocess_explanation(explanation)\n    axes[2, i].imshow(original_img)\n    axes[2, i].imshow(processed, alpha=0.5, cmap='hot')\n    axes[2, i].set_title(label)\n    axes[2, i].axis('off')\n\n# Hide unused subplot\naxes[2, 2].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Working with Custom Model Architectures\n\nSignXAI2's unified API works seamlessly with custom TensorFlow model architectures. Let's create a simple custom model and explain its predictions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the custom model\n",
    "# (Since we didn't train it, it'll just make random predictions)\n",
    "custom_prediction = custom_model.predict(preprocessed_img)\n",
    "custom_predicted_class = np.argmax(custom_prediction[0])\n",
    "print(f\"Custom model prediction: Class {custom_predicted_class}\")\n",
    "\n",
    "# Apply different explainability methods to the custom model\n",
    "methods = {\n",
    "    \"Gradient\": SIGN(custom_model_no_softmax),\n",
    "    \"GradCAM\": GradCAM(custom_model, layer_name='conv2d_2'),  # Last conv layer\n",
    "    \"GuidedBackprop\": GuidedBackprop(custom_model_no_softmax),\n",
    "}\n",
    "\n",
    "custom_explanations = {}\n",
    "for name, method in methods.items():\n",
    "    explanation = method.attribute(preprocessed_img, target_class=custom_predicted_class).numpy()\n",
    "    custom_explanations[name] = explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Make a prediction with the custom model\n# (Since we didn't train it, it'll just make random predictions)\ncustom_prediction = custom_model.predict(preprocessed_img)\ncustom_predicted_class = np.argmax(custom_prediction[0])\nprint(f\"Custom model prediction: Class {custom_predicted_class}\")\n\n# Apply different explainability methods to the custom model using the unified API\ncustom_methods = {\n    'gradient': {},\n    'grad_cam': {'layer_name': 'conv2d_2'},  # Last conv layer\n    'guided_backprop': {},\n    'smoothgrad': {'noise_scale': 0.1, 'augment_by_n': 25}\n}\n\ncustom_explanations = {}\nfor method_name, params in custom_methods.items():\n    print(f\"Generating {method_name} explanation for custom model...\")\n    explanation = explain(\n        model=custom_model,\n        x=preprocessed_img,\n        method_name=method_name,\n        target_class=custom_predicted_class,\n        **params\n    )\n    custom_explanations[method_name] = explanation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Visualize explanations for the custom model\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\nfor i, (name, explanation) in enumerate(custom_explanations.items()):\n    processed = preprocess_explanation(explanation)\n    axes[i].imshow(original_img)\n    axes[i].imshow(processed, alpha=0.5, cmap='hot')\n    axes[i].set_title(f\"Custom Model: {name.replace('_', ' ').title()}\")\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Advanced Visualization Techniques\n\nSignXAI2 provides various ways to visualize explanations. Let's explore different visualization options and techniques."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get a high-quality explanation to visualize\nlrp_explanation = explain(\n    model=model,\n    x=preprocessed_img,\n    method_name='lrp_epsilon_0_1',\n    target_class=predicted_class\n)\nlrp_processed = preprocess_explanation(lrp_explanation)\n\n# 1. Visualization with different color maps and overlays\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Row 1: Different colormaps\ncolormaps = ['hot', 'jet', 'RdBu_r']\nfor i, cmap in enumerate(colormaps):\n    axes[0, i].imshow(original_img)\n    im = axes[0, i].imshow(lrp_processed, alpha=0.5, cmap=cmap)\n    axes[0, i].set_title(f\"{cmap} colormap\")\n    axes[0, i].axis('off')\n\n# Row 2: Different overlay techniques\n# Pure heatmap\naxes[1, 0].imshow(lrp_processed, cmap='hot')\naxes[1, 0].set_title(\"Pure Heatmap\")\naxes[1, 0].axis('off')\n\n# Masked overlay (only show high relevance areas)\nthreshold = 0.3\nmask = (lrp_processed.mean(axis=2) > threshold)\nmasked_img = original_img.copy()\nmasked_img[~mask] = masked_img[~mask] * 0.3  # Dim irrelevant areas\naxes[1, 1].imshow(masked_img)\naxes[1, 1].set_title(f\"Masked (threshold={threshold})\")\naxes[1, 1].axis('off')\n\n# Contour overlay\nfrom skimage import measure\ncontours = measure.find_contours(lrp_processed.mean(axis=2), 0.3)\naxes[1, 2].imshow(original_img)\nfor contour in contours:\n    axes[1, 2].plot(contour[:, 1], contour[:, 0], linewidth=2, color='red')\naxes[1, 2].set_title(\"Contour Overlay\")\naxes[1, 2].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integrating with TensorFlow's GradientTape API\n",
    "\n",
    "You can create your own custom explainability methods using TensorFlow's GradientTape for more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize integrated gradients explanation compared to standard gradient\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Standard gradient\n",
    "visualize_attribution(np_img, preprocess_explanation(gradient_explanation), ax=axes[0])\n",
    "axes[0].set_title(\"Standard Gradient\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Integrated gradients\n",
    "visualize_attribution(np_img, preprocess_explanation(ig_explanation), ax=axes[1])\n",
    "axes[1].set_title(\"Integrated Gradients (20 steps)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Generate multiple explanations\nensemble_methods = ['gradient', 'guided_backprop', 'lrp_epsilon_0_1', 'smoothgrad']\nensemble_explanations = {}\n\nfor method in ensemble_methods:\n    print(f\"Generating {method} explanation...\")\n    explanation = explain(\n        model=model,\n        x=preprocessed_img,\n        method_name=method,\n        target_class=predicted_class,\n        noise_scale=0.1 if method == 'smoothgrad' else None,\n        augment_by_n=25 if method == 'smoothgrad' else None\n    )\n    ensemble_explanations[method] = preprocess_explanation(explanation)\n\n# Create ensemble explanation by averaging\nensemble_avg = np.mean(list(ensemble_explanations.values()), axis=0)\n\n# Create ensemble explanation by taking maximum\nensemble_max = np.max(list(ensemble_explanations.values()), axis=0)\n\n# Visualize individual and ensemble explanations\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Row 1: Individual methods\nfor i, (method, explanation) in enumerate(list(ensemble_explanations.items())[:3]):\n    axes[0, i].imshow(original_img)\n    axes[0, i].imshow(explanation, alpha=0.5, cmap='hot')\n    axes[0, i].set_title(method.replace('_', ' ').title())\n    axes[0, i].axis('off')\n\n# Row 2: Last method and ensemble results\naxes[1, 0].imshow(original_img)\naxes[1, 0].imshow(list(ensemble_explanations.values())[3], alpha=0.5, cmap='hot')\naxes[1, 0].set_title(list(ensemble_explanations.keys())[3].replace('_', ' ').title())\naxes[1, 0].axis('off')\n\naxes[1, 1].imshow(original_img)\naxes[1, 1].imshow(ensemble_avg, alpha=0.5, cmap='hot')\naxes[1, 1].set_title(\"Ensemble (Average)\")\naxes[1, 1].axis('off')\n\naxes[1, 2].imshow(original_img)\naxes[1, 2].imshow(ensemble_max, alpha=0.5, cmap='hot')\naxes[1, 2].set_title(\"Ensemble (Maximum)\")\naxes[1, 2].axis('off')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Conclusion\n\nIn this advanced tutorial, we explored several sophisticated aspects of using SignXAI2 with TensorFlow:\n\n1. **LRP Variants**: We demonstrated how to use different LRP methods (Z-rule, epsilon-rule, alpha-beta rules, and SIGN variants) through the unified API.\n\n2. **Advanced Parameters**: We showed how to customize explanation methods with various parameters like noise levels for SmoothGrad, step counts for Integrated Gradients, and layer selection for Grad-CAM.\n\n3. **Custom Models**: We demonstrated that SignXAI2's unified API works seamlessly with custom model architectures.\n\n4. **Advanced Visualization**: We explored different visualization techniques including various colormaps, masking, and contour overlays.\n\n5. **Ensemble Methods**: We showed how to combine multiple explanation methods to get more robust insights.\n\nThe unified API in SignXAI2 makes it easy to:\n- Switch between different explanation methods\n- Compare results across methods\n- Work with both pre-trained and custom models\n- Customize parameters for fine-tuned explanations\n\nFor more information and to contribute to the project, visit the [SignXAI2 GitHub repository](https://github.com/IRISlaboratory/signxai2).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}