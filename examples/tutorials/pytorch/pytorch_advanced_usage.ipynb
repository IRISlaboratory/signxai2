{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SignXAI with PyTorch - Advanced Usage\n",
    "\n",
    "This tutorial demonstrates advanced techniques for using SignXAI with PyTorch models. It builds on the basic tutorial and explores more sophisticated explainability methods and customizations.\n",
    "\n",
    "## Setup Requirements\n",
    "\n",
    "For this PyTorch tutorial, you'll need to install SignXAI with PyTorch dependencies:\n",
    "\n",
    "```bash\n",
    "# For conda users\n",
    "conda create -n signxai-pytorch python=3.8\n",
    "conda activate signxai-pytorch\n",
    "pip install -r ../../requirements/common.txt\n",
    "pip install -r ../../requirements/pytorch.txt\n",
    "\n",
    "# Or for pip users\n",
    "python -m venv signxai_pytorch_env\n",
    "source signxai_pytorch_env/bin/activate  # On Windows: signxai_pytorch_env\\Scripts\\activate\n",
    "pip install -r ../../requirements/common.txt\n",
    "pip install -r ../../requirements/pytorch.txt\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "1. Customizing LRP rules with PyTorch\n",
    "2. Creating composite explanations\n",
    "3. Working with custom model architectures\n",
    "4. Advanced visualization techniques\n",
    "5. Layer-wise relevance tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# SignXAI imports - including advanced modules\n",
    "from signxai.torch_signxai.methods import (\n",
    "    SIGN, \n",
    "    GradCAM, \n",
    "    GuidedBackprop, \n",
    "    LRPZ, \n",
    "    LRPEpsilon,\n",
    "    LRPAlphaBeta,  # Alpha-Beta rule for LRP\n",
    "    LRPComposite   # For composite LRP rules\n",
    ")\n",
    "from signxai.common.visualization import visualize_attribution\n",
    "from signxai.torch_signxai.utils import remove_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and model paths\n",
    "_THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "DATA_DIR = os.path.realpath(os.path.join(_THIS_DIR, \"..\", \"..\", \"data\"))\n",
    "VGG16_MODEL_PATH = os.path.join(DATA_DIR, \"models\", \"pytorch\", \"VGG16\", \"vgg16_torch.pt\")\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, \"images\", \"example.jpg\")\n",
    "\n",
    "# Verify paths\n",
    "assert os.path.exists(VGG16_MODEL_PATH), f\"VGG16 model not found at {VGG16_MODEL_PATH}\"\n",
    "assert os.path.exists(IMAGE_PATH), f\"Image not found at {IMAGE_PATH}\"\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = torch.load(VGG16_MODEL_PATH)\n",
    "    print(\"Loaded pre-saved VGG16 model\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load saved model: {e}\\nLoading from torchvision instead\")\n",
    "    import torchvision.models as models\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    \n",
    "model.eval()  # Set to evaluation mode\n",
    "model_no_softmax = remove_softmax(model)\n",
    "\n",
    "# Helper function to load and preprocess image\n",
    "def preprocess_image(image_path, size=(224, 224)):\n",
    "    img = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return img, transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load and display image\n",
    "original_img, preprocessed_img = preprocess_image(IMAGE_PATH)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(original_img)\n",
    "plt.axis('off')\n",
    "plt.title('Sample Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customizing LRP Rules with PyTorch\n",
    "\n",
    "SignXAI provides extensive customization options for Layer-wise Relevance Propagation (LRP). Let's explore how to configure different rules for different layers of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prediction for reference\n",
    "with torch.no_grad():\n",
    "    output = model(preprocessed_img)\n",
    "    _, predicted_idx = torch.max(output, 1)\n",
    "    predicted_class = predicted_idx.item()\n",
    "    \n",
    "print(f\"Predicted class index: {predicted_class}\")\n",
    "\n",
    "# Example 1: Using LRP Alpha-Beta rule with custom alpha/beta values\n",
    "alpha, beta = 2, 1  # Alpha-Beta parameters (alpha - beta = 1 to ensure conservation)\n",
    "lrp_alpha_beta = LRPAlphaBeta(model_no_softmax, alpha=alpha, beta=beta)\n",
    "explanation_alpha_beta = lrp_alpha_beta.attribute(preprocessed_img, target=predicted_class)\n",
    "\n",
    "# Example 2: Using LRP Epsilon rule with a custom epsilon value\n",
    "epsilon = 0.01  # Small epsilon for sharper attribution maps\n",
    "lrp_epsilon = LRPEpsilon(model_no_softmax, epsilon=epsilon)\n",
    "explanation_epsilon = lrp_epsilon.attribute(preprocessed_img, target=predicted_class)\n",
    "\n",
    "# Example 3: Composite LRP - Different rules for different layers\n",
    "# Define layer-wise rules\n",
    "layer_rules = {\n",
    "    # Format: 'layer_name_or_regex': ('rule_name', rule_params)\n",
    "    'features.0': ('epsilon', {'epsilon': 0.1}),  # First conv layer - epsilon rule\n",
    "    'features.*': ('alpha_beta', {'alpha': 1, 'beta': 0}),  # Middle layers - alpha1-beta0\n",
    "    'classifier.*': ('z_plus', {})  # Final layers - z+ rule\n",
    "}\n",
    "\n",
    "# Create composite LRP explainer\n",
    "lrp_composite = LRPComposite(model_no_softmax, layer_rules=layer_rules)\n",
    "explanation_composite = lrp_composite.attribute(preprocessed_img, target=predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the different LRP variants\n",
    "def preprocess_for_visualization(explanation):\n",
    "    # Handle PyTorch tensor shape [B, C, H, W]\n",
    "    if isinstance(explanation, torch.Tensor):\n",
    "        explanation = explanation.detach().cpu().numpy()\n",
    "    if explanation.ndim == 4:\n",
    "        explanation = np.transpose(explanation[0], (1, 2, 0))  # [H, W, C]\n",
    "    elif explanation.ndim == 3 and explanation.shape[0] == 3:  # If it's [C, H, W]\n",
    "        explanation = np.transpose(explanation, (1, 2, 0))      # [H, W, C]\n",
    "    \n",
    "    # Use absolute values for gradient-based methods\n",
    "    abs_explanation = np.abs(explanation)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    if abs_explanation.max() > 0:\n",
    "        abs_explanation = abs_explanation / abs_explanation.max()\n",
    "    \n",
    "    return abs_explanation\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Get numpy array of original image\n",
    "np_img = np.array(original_img.resize((224, 224)))\n",
    "\n",
    "# Plot the different LRP variants\n",
    "visualize_attribution(np_img, preprocess_for_visualization(explanation_alpha_beta), ax=axes[0])\n",
    "axes[0].set_title(f\"LRP Alpha-Beta (α={alpha}, β={beta})\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "visualize_attribution(np_img, preprocess_for_visualization(explanation_epsilon), ax=axes[1])\n",
    "axes[1].set_title(f\"LRP Epsilon (ε={epsilon})\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "visualize_attribution(np_img, preprocess_for_visualization(explanation_composite), ax=axes[2])\n",
    "axes[2].set_title(\"LRP Composite (Layer-wise rules)\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Composite Explanations\n",
    "\n",
    "You can combine multiple explainability methods to create composite explanations that highlight different aspects of the model's decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the target layer for GradCAM\n",
    "target_layer = None\n",
    "for name, module in model.features.named_children():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        target_layer = module  # Get the last convolutional layer\n",
    "\n",
    "# Generate explanations using different methods\n",
    "gradcam = GradCAM(model_no_softmax, target_layer)\n",
    "gradcam_explanation = gradcam.attribute(preprocessed_img, target=predicted_class)\n",
    "\n",
    "guided_bp = GuidedBackprop(model_no_softmax)\n",
    "guided_bp_explanation = guided_bp.attribute(preprocessed_img, target=predicted_class)\n",
    "\n",
    "# Create Guided GradCAM as a composite of GradCAM and Guided Backpropagation\n",
    "# Convert to numpy for processing\n",
    "gradcam_np = gradcam_explanation.detach().cpu().numpy()\n",
    "guided_bp_np = guided_bp_explanation.detach().cpu().numpy()\n",
    "\n",
    "# Reshape GradCAM to match Guided Backprop dimensions if needed\n",
    "if gradcam_np.shape != guided_bp_np.shape:\n",
    "    # Assuming GradCAM is [1, 1, H, W] and Guided BP is [1, 3, H, W]\n",
    "    gradcam_reshaped = np.repeat(gradcam_np, 3, axis=1)\n",
    "    guided_gradcam = guided_bp_np * gradcam_reshaped\n",
    "else:\n",
    "    guided_gradcam = guided_bp_np * gradcam_np\n",
    "\n",
    "# Create a gradient-weighted LRP by combining gradient and LRP\n",
    "gradient = SIGN(model_no_softmax)\n",
    "gradient_explanation = gradient.attribute(preprocessed_img, target=predicted_class).detach().cpu().numpy()\n",
    "lrp_explanation = explanation_epsilon.detach().cpu().numpy()\n",
    "\n",
    "# Weight LRP by gradient magnitude\n",
    "gradient_weighted_lrp = gradient_explanation * lrp_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the composite explanations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot original methods and composites\n",
    "visualize_attribution(np_img, preprocess_for_visualization(guided_bp_np), ax=axes[0])\n",
    "axes[0].set_title(\"Guided Backpropagation\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "visualize_attribution(np_img, preprocess_for_visualization(gradcam_np), ax=axes[1])\n",
    "axes[1].set_title(\"GradCAM\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "visualize_attribution(np_img, preprocess_for_visualization(guided_gradcam), ax=axes[2])\n",
    "axes[2].set_title(\"Guided GradCAM (Composite)\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "visualize_attribution(np_img, preprocess_for_visualization(gradient_weighted_lrp), ax=axes[3])\n",
    "axes[3].set_title(\"Gradient-Weighted LRP (Composite)\")\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Working with Custom Model Architectures\n",
    "\n",
    "SignXAI works with custom PyTorch model architectures, not just pre-defined ones. Let's create a simple custom model and explain its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple custom CNN model\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # Feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize custom model\n",
    "custom_model = CustomCNN()\n",
    "custom_model.eval()\n",
    "\n",
    "# For demo purposes, we'll use random weights\n",
    "# In a real scenario, you'd train this model first\n",
    "\n",
    "# Remove softmax from the custom model\n",
    "custom_model_no_softmax = remove_softmax(custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a forward pass to get a random prediction\n",
    "with torch.no_grad():\n",
    "    custom_output = custom_model(preprocessed_img)\n",
    "    _, custom_predicted_idx = torch.max(custom_output, 1)\n",
    "    custom_predicted_class = custom_predicted_idx.item()\n",
    "    \n",
    "print(f\"Custom model prediction (random weights): Class {custom_predicted_class}\")\n",
    "\n",
    "# Apply explainability methods to the custom model\n",
    "custom_last_conv = custom_model.features[6]  # Last conv layer\n",
    "\n",
    "custom_methods = {\n",
    "    \"Gradient\": SIGN(custom_model_no_softmax),\n",
    "    \"GradCAM\": GradCAM(custom_model_no_softmax, custom_last_conv),\n",
    "    \"LRP-Z\": LRPZ(custom_model_no_softmax),\n",
    "}\n",
    "\n",
    "custom_explanations = {}\n",
    "for name, method in custom_methods.items():\n",
    "    explanation = method.attribute(preprocessed_img, target=custom_predicted_class)\n",
    "    custom_explanations[name] = explanation.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explanations for the custom model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, (name, explanation) in enumerate(custom_explanations.items()):\n",
    "    visualize_attribution(np_img, preprocess_for_visualization(explanation), ax=axes[i])\n",
    "    axes[i].set_title(f\"Custom Model: {name}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Visualization Techniques\n",
    "\n",
    "SignXAI provides advanced visualization options to help interpret complex models more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a high-quality explanation to visualize\n",
    "lrp = LRPZ(model_no_softmax)\n",
    "lrp_explanation = lrp.attribute(preprocessed_img, target=predicted_class).detach().cpu().numpy()\n",
    "lrp_processed = preprocess_for_visualization(lrp_explanation)\n",
    "\n",
    "# 1. Visualization with different color maps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Default Red-Blue colormap\n",
    "visualize_attribution(np_img, lrp_processed, ax=axes[0], cmap='RdBu_r')\n",
    "axes[0].set_title(\"Red-Blue Colormap\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Heat colormap\n",
    "visualize_attribution(np_img, lrp_processed, ax=axes[1], cmap='hot')\n",
    "axes[1].set_title(\"Heat Colormap\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Viridis colormap\n",
    "visualize_attribution(np_img, lrp_processed, ax=axes[2], cmap='viridis')\n",
    "axes[2].set_title(\"Viridis Colormap\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visualization with different alpha (transparency) values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Low alpha (more transparent)\n",
    "visualize_attribution(np_img, lrp_processed, ax=axes[0], alpha=0.3)\n",
    "axes[0].set_title(\"Low Transparency (α=0.3)\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Medium alpha\n",
    "visualize_attribution(np_img, lrp_processed, ax=axes[1], alpha=0.6)\n",
    "axes[1].set_title(\"Medium Transparency (α=0.6)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# High alpha (less transparent)\n",
    "visualize_attribution(np_img, lrp_processed, ax=axes[2], alpha=0.9)\n",
    "axes[2].set_title(\"High Transparency (α=0.9)\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Advanced: Custom masking visualization\n",
    "# We'll create a visualization that only shows areas with high attribution\n",
    "\n",
    "# Create a binary mask based on attribution threshold\n",
    "threshold = 0.5  # Threshold for significance\n",
    "binary_mask = (lrp_processed > threshold).astype(float)\n",
    "\n",
    "# Apply mask to the original image\n",
    "masked_img = np_img.copy().astype(float) / 255.0\n",
    "for i in range(3):  # Apply to each color channel\n",
    "    if binary_mask.ndim == 3:\n",
    "        channel_mask = binary_mask[:, :, i]\n",
    "    else:\n",
    "        channel_mask = binary_mask\n",
    "    masked_img[:, :, i] = masked_img[:, :, i] * channel_mask\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(np_img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Binary mask\n",
    "axes[1].imshow(binary_mask, cmap='gray')\n",
    "axes[1].set_title(f\"Binary Mask (threshold={threshold})\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Masked image\n",
    "axes[2].imshow(masked_img)\n",
    "axes[2].set_title(\"Masked Image (Only Important Features)\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Layer-wise Relevance Tracking\n",
    "\n",
    "Advanced users might want to track how relevance flows through different layers of the network. SignXAI allows you to access intermediate relevance maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom LRP analyzer that tracks layer-wise relevance\n",
    "class LayerTrackingLRP(LRPZ):\n",
    "    def __init__(self, model, tracked_layers=None):\n",
    "        super().__init__(model)\n",
    "        self.tracked_layers = tracked_layers or []\n",
    "        self.layer_relevances = {}\n",
    "        \n",
    "        # Register hooks for tracking\n",
    "        self.hooks = []\n",
    "        for name, module in model.named_modules():\n",
    "            if any(tracked in name for tracked in self.tracked_layers):\n",
    "                hook = module.register_forward_hook(self._hook_fn(name))\n",
    "                self.hooks.append(hook)\n",
    "    \n",
    "    def _hook_fn(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.layer_relevances[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def __del__(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "# For VGG16, track feature layers at different depths\n",
    "tracked_layers = [\n",
    "    'features.1',    # Early layer (after first ReLU)\n",
    "    'features.15',   # Middle layer\n",
    "    'features.28',   # Deep layer (last feature layer)\n",
    "    'classifier.3'   # Final classifier layer\n",
    "]\n",
    "\n",
    "# Create the layer tracking analyzer\n",
    "tracking_lrp = LayerTrackingLRP(model_no_softmax, tracked_layers=tracked_layers)\n",
    "\n",
    "# Generate explanation\n",
    "tracking_explanation = tracking_lrp.attribute(preprocessed_img, target=predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intermediate layer activations\n",
    "# For this example, we'll just visualize the first few channels of each layer\n",
    "\n",
    "# Function to visualize activation maps\n",
    "def visualize_activations(activations, max_channels=4, title=\"Layer Activations\"):\n",
    "    if len(activations) == 0:\n",
    "        return\n",
    "    \n",
    "    # Determine plot size\n",
    "    num_layers = len(activations)\n",
    "    channels_per_layer = min(max_channels, activations[0].shape[1])\n",
    "    \n",
    "    fig, axes = plt.subplots(num_layers, channels_per_layer, figsize=(4*channels_per_layer, 3*num_layers))\n",
    "    \n",
    "    for i, (layer_name, activation) in enumerate(activations.items()):\n",
    "        act = activation.cpu().numpy()\n",
    "        for j in range(channels_per_layer):\n",
    "            ax = axes[i, j] if num_layers > 1 else axes[j]\n",
    "            channel_data = act[0, j]\n",
    "            \n",
    "            # Normalize for visualization\n",
    "            if channel_data.max() > channel_data.min():\n",
    "                channel_data = (channel_data - channel_data.min()) / (channel_data.max() - channel_data.min())\n",
    "                \n",
    "            ax.imshow(channel_data, cmap='viridis')\n",
    "            ax.set_title(f\"{layer_name}\\nChannel {j}\")\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)  # Adjust for suptitle\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the tracked layer activations\n",
    "visualize_activations(tracking_lrp.layer_relevances, title=\"Layer-wise Activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this advanced tutorial, we explored several sophisticated aspects of using SignXAI with PyTorch:\n",
    "\n",
    "1. **Customizing LRP Rules**: We demonstrated how to configure different LRP variants and create composite rules for different layers.\n",
    "\n",
    "2. **Creating Composite Explanations**: We combined multiple explainability methods to create enhanced visualizations like Guided GradCAM.\n",
    "\n",
    "3. **Working with Custom Models**: We showed how SignXAI works seamlessly with custom model architectures.\n",
    "\n",
    "4. **Advanced Visualization**: We explored different visualization options including different colormaps, transparency settings, and masking techniques.\n",
    "\n",
    "5. **Layer-wise Relevance Tracking**: We demonstrated how to track and visualize relevance flow through different layers of the network.\n",
    "\n",
    "These advanced techniques provide deeper insights into how your models make decisions and can help identify potential biases or weaknesses in the model's reasoning process.\n",
    "\n",
    "For more information and to contribute to the project, visit the SignXAI documentation and GitHub repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}