{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SignXAI2 with PyTorch - Advanced Usage\n\nThis tutorial demonstrates advanced techniques for using SignXAI2 with PyTorch models. It builds on the basic tutorial and explores more sophisticated explainability methods and customizations.\n\n## Setup Requirements\n\n**Important**: SignXAI2 requires Python 3.9 or 3.10 (Python 3.11+ is not supported)\n\nSince you're running this tutorial, you should already have cloned the signxai2 repository. From the repository root directory:\n\n### Using conda:\n```bash\n# Create environment with Python 3.10\nconda create -n signxai2 python=3.10\nconda activate signxai2\n\n# Install SignXAI2 with PyTorch support\npip install signxai2[pytorch]\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\n### Using venv:\n```bash\n# Create virtual environment\npython3.10 -m venv signxai2_env\nsource signxai2_env/bin/activate  # On Windows: signxai2_env\\Scripts\\activate\n\n# Install SignXAI2 with PyTorch support\npip install signxai2[pytorch]\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\n## Overview\n\nIn this tutorial, we'll cover:\n1. Using different LRP variants with the unified API\n2. Advanced method parameters and configurations\n3. Working with custom model architectures\n4. Combining multiple explanation methods\n5. Advanced visualization techniques"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\n# SignXAI2 unified API imports\nfrom signxai import explain, list_methods, get_method_info"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up data and model paths\n_THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\nDATA_DIR = os.path.realpath(os.path.join(_THIS_DIR, \"..\", \"..\", \"data\"))\nVGG16_MODEL_PATH = os.path.join(DATA_DIR, \"models\", \"pytorch\", \"VGG16\", \"vgg16_ported_weights.pt\")\nIMAGE_PATH = os.path.join(DATA_DIR, \"images\", \"example.jpg\")\n\n# Verify paths\nassert os.path.exists(VGG16_MODEL_PATH), f\"VGG16 model not found at {VGG16_MODEL_PATH}\"\nassert os.path.exists(IMAGE_PATH), f\"Image not found at {IMAGE_PATH}\"\n\n# Import the VGG16 model definition\nsys.path.append(os.path.join(DATA_DIR, \"models\", \"pytorch\", \"VGG16\"))\nfrom VGG16 import VGG16_PyTorch\n\n# Load model\ntry:\n    # Initialize the model architecture\n    model = VGG16_PyTorch(num_classes=1000)\n    # Load the pre-trained weights\n    model.load_state_dict(torch.load(VGG16_MODEL_PATH, map_location=torch.device('cpu')))\n    print(\"Loaded pre-saved VGG16 model weights\")\nexcept Exception as e:\n    print(f\"Could not load saved model: {e}\\nLoading from torchvision instead\")\n    import torchvision.models as models\n    model = models.vgg16(pretrained=True)\n    \nmodel.eval()  # Set to evaluation mode\n\n# Helper function to load and preprocess image\ndef preprocess_image(image_path, size=(224, 224)):\n    img = Image.open(image_path)\n    transform = transforms.Compose([\n        transforms.Resize(size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    return img, transform(img).unsqueeze(0)  # Add batch dimension\n\n# Load and display image\noriginal_img, preprocessed_img = preprocess_image(IMAGE_PATH)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(original_img)\nplt.axis('off')\nplt.title('Sample Image')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Using Different LRP Variants with the Unified API\n\nSignXAI2 provides various LRP (Layer-wise Relevance Propagation) methods through the unified API. Let's explore different LRP rules and configurations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get a prediction for reference\nwith torch.no_grad():\n    output = model(preprocessed_img)\n    _, predicted_idx = torch.max(output, 1)\n    predicted_class = predicted_idx.item()\n    \nprint(f\"Predicted class index: {predicted_class}\")\n\n# Different LRP variants available in SignXAI2\nlrp_methods = [\n    'lrp_z',\n    'lrp_epsilon_0_001',\n    'lrp_epsilon_0_01', \n    'lrp_epsilon_0_1',\n    'lrp_epsilon_0_2',\n    'lrp_alpha_1_beta_0',\n    'lrp_alpha_2_beta_1',\n    'lrpsign_z',\n    'lrpsign_epsilon_0_1'\n]\n\n# Generate explanations using different LRP variants\nlrp_explanations = {}\nfor method in lrp_methods:\n    if method in list_methods():\n        print(f\"Generating {method} explanation...\")\n        explanation = explain(\n            model=model,\n            x=preprocessed_img,\n            method_name=method,\n            target_class=predicted_class\n        )\n        lrp_explanations[method] = explanation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the different LRP variants\ndef preprocess_for_visualization(explanation):\n    # Handle PyTorch tensor shape [B, C, H, W]\n    if isinstance(explanation, torch.Tensor):\n        explanation = explanation.detach().cpu().numpy()\n    if explanation.ndim == 4:\n        explanation = np.transpose(explanation[0], (1, 2, 0))  # [H, W, C]\n    elif explanation.ndim == 3 and explanation.shape[0] == 3:  # If it's [C, H, W]\n        explanation = np.transpose(explanation, (1, 2, 0))      # [H, W, C]\n    \n    # Convert single channel to RGB\n    if explanation.ndim == 2:\n        explanation = np.expand_dims(explanation, axis=-1)\n        explanation = np.repeat(explanation, 3, axis=-1)\n    elif explanation.shape[-1] == 1:\n        explanation = np.repeat(explanation, 3, axis=-1)\n    \n    # Use absolute values for gradient-based methods\n    abs_explanation = np.abs(explanation)\n    \n    # Normalize for visualization\n    if abs_explanation.max() > 0:\n        abs_explanation = abs_explanation / abs_explanation.max()\n    \n    return abs_explanation\n\n# Create a figure for visualization\nn_methods = len(lrp_explanations)\nn_cols = 3\nn_rows = (n_methods + n_cols - 1) // n_cols\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6*n_rows))\naxes = axes.flatten() if n_rows > 1 else [axes]\n\n# Get numpy array of original image\nnp_img = np.array(original_img.resize((224, 224)))\n\n# Plot the different LRP variants\nfor i, (method_name, explanation) in enumerate(lrp_explanations.items()):\n    if i < len(axes):\n        processed_explanation = preprocess_for_visualization(explanation)\n        axes[i].imshow(np_img)\n        axes[i].imshow(processed_explanation, alpha=0.5, cmap='hot')\n        axes[i].set_title(method_name.replace('_', ' ').upper())\n        axes[i].axis('off')\n\n# Hide unused subplots\nfor i in range(len(lrp_explanations), len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Advanced Method Parameters and Configurations\n\nSignXAI2's unified API allows you to customize explanation methods with various parameters. Let's explore some advanced configurations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: SmoothGrad with different noise levels\nsmoothgrad_params = [\n    {'noise_level': 0.05, 'num_samples': 10},\n    {'noise_level': 0.1, 'num_samples': 25},\n    {'noise_level': 0.2, 'num_samples': 50}\n]\n\nsmoothgrad_explanations = {}\nfor params in smoothgrad_params:\n    label = f\"SmoothGrad (noise={params['noise_level']}, n={params['num_samples']})\"\n    print(f\"Generating {label}...\")\n    explanation = explain(\n        model=model,\n        x=preprocessed_img,\n        method_name='smoothgrad',\n        target_class=predicted_class,\n        **params\n    )\n    smoothgrad_explanations[label] = explanation\n\n# Example 2: Integrated Gradients with different step counts\nig_steps = [10, 25, 50]\nig_explanations = {}\nfor steps in ig_steps:\n    label = f\"Integrated Gradients ({steps} steps)\"\n    print(f\"Generating {label}...\")\n    explanation = explain(\n        model=model,\n        x=preprocessed_img,\n        method_name='integrated_gradients',\n        target_class=predicted_class,\n        ig_steps=steps  # PyTorch uses 'ig_steps' parameter\n    )\n    ig_explanations[label] = explanation\n\n# Example 3: Grad-CAM with different layers\n# Find convolutional layers in VGG16\nconv_layers = []\nfor name, module in model.features.named_children():\n    if isinstance(module, nn.Conv2d):\n        conv_layers.append(f\"features.{name}\")\n\n# Select a few layers for comparison\nselected_layers = [conv_layers[-3], conv_layers[-1]]  # Third-to-last and last conv layers\ngradcam_explanations = {}\nfor layer in selected_layers:\n    label = f\"Grad-CAM ({layer})\"\n    print(f\"Generating {label}...\")\n    explanation = explain(\n        model=model,\n        x=preprocessed_img,\n        method_name='grad_cam',\n        target_class=predicted_class,\n        target_layer=layer  # PyTorch uses 'target_layer'\n    )\n    gradcam_explanations[label] = explanation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize parameter variations\nfig, axes = plt.subplots(3, 3, figsize=(18, 18))\n\n# Row 1: SmoothGrad variations\nfor i, (label, explanation) in enumerate(smoothgrad_explanations.items()):\n    processed = preprocess_for_visualization(explanation)\n    axes[0, i].imshow(np_img)\n    axes[0, i].imshow(processed, alpha=0.5, cmap='hot')\n    axes[0, i].set_title(label)\n    axes[0, i].axis('off')\n\n# Row 2: Integrated Gradients variations\nfor i, (label, explanation) in enumerate(ig_explanations.items()):\n    processed = preprocess_for_visualization(explanation)\n    axes[1, i].imshow(np_img)\n    axes[1, i].imshow(processed, alpha=0.5, cmap='hot')\n    axes[1, i].set_title(label)\n    axes[1, i].axis('off')\n\n# Row 3: Grad-CAM variations\nfor i, (label, explanation) in enumerate(gradcam_explanations.items()):\n    processed = preprocess_for_visualization(explanation)\n    axes[2, i].imshow(np_img)\n    axes[2, i].imshow(processed, alpha=0.5, cmap='hot')\n    axes[2, i].set_title(label)\n    axes[2, i].axis('off')\n\n# Hide unused subplot\naxes[2, 2].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Working with Custom Model Architectures\n\nSignXAI2's unified API works seamlessly with custom PyTorch model architectures. Let's create a simple custom model and explain its predictions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple custom CNN model\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # Feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize custom model\n",
    "custom_model = CustomCNN()\n",
    "custom_model.eval()\n",
    "\n",
    "# For demo purposes, we'll use random weights\n",
    "# In a real scenario, you'd train this model first\n",
    "\n",
    "# Remove softmax from the custom model\n",
    "custom_model_no_softmax = remove_softmax(custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate a forward pass to get a random prediction\nwith torch.no_grad():\n    custom_output = custom_model(preprocessed_img)\n    _, custom_predicted_idx = torch.max(custom_output, 1)\n    custom_predicted_class = custom_predicted_idx.item()\n    \nprint(f\"Custom model prediction (random weights): Class {custom_predicted_class}\")\n\n# Apply explainability methods to the custom model using the unified API\n# Find the last conv layer for GradCAM\ncustom_last_conv = None\nfor module in custom_model.features.children():\n    if isinstance(module, nn.Conv2d):\n        custom_last_conv = module\n\ncustom_methods = {\n    'gradient': {},\n    'grad_cam': {'target_layer': 'features.6'},  # Last conv layer in our custom model\n    'guided_backprop': {},\n    'lrp_z': {},\n    'smoothgrad': {'noise_level': 0.1, 'num_samples': 25}\n}\n\ncustom_explanations = {}\nfor method_name, params in custom_methods.items():\n    print(f\"Generating {method_name} explanation for custom model...\")\n    explanation = explain(\n        model=custom_model,\n        x=preprocessed_img,\n        method_name=method_name,\n        target_class=custom_predicted_class,\n        **params\n    )\n    custom_explanations[method_name] = explanation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize explanations for the custom model\nn_methods = len(custom_explanations)\nfig, axes = plt.subplots(1, n_methods, figsize=(4*n_methods, 4))\n\nif n_methods == 1:\n    axes = [axes]\n\nfor i, (name, explanation) in enumerate(custom_explanations.items()):\n    processed = preprocess_for_visualization(explanation)\n    axes[i].imshow(np_img)\n    axes[i].imshow(processed, alpha=0.5, cmap='hot')\n    axes[i].set_title(f\"Custom Model: {name.replace('_', ' ').title()}\")\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Combining Multiple Explanation Methods\n\nSometimes it's useful to combine insights from multiple explanation methods. Let's create ensemble explanations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate multiple explanations for ensemble\nensemble_methods = ['gradient', 'guided_backprop', 'lrp_epsilon_0_1', 'smoothgrad']\nensemble_explanations = {}\n\nfor method in ensemble_methods:\n    print(f\"Generating {method} explanation...\")\n    params = {}\n    if method == 'smoothgrad':\n        params = {'noise_level': 0.1, 'num_samples': 25}\n    \n    explanation = explain(\n        model=model,\n        x=preprocessed_img,\n        method_name=method,\n        target_class=predicted_class,\n        **params\n    )\n    ensemble_explanations[method] = preprocess_for_visualization(explanation)\n\n# Create ensemble explanation by averaging\nensemble_avg = np.mean(list(ensemble_explanations.values()), axis=0)\n\n# Create ensemble explanation by taking maximum\nensemble_max = np.max(list(ensemble_explanations.values()), axis=0)\n\n# Visualize individual and ensemble explanations\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Row 1: Individual methods\nfor i, (method, explanation) in enumerate(list(ensemble_explanations.items())[:3]):\n    axes[0, i].imshow(np_img)\n    axes[0, i].imshow(explanation, alpha=0.5, cmap='hot')\n    axes[0, i].set_title(method.replace('_', ' ').title())\n    axes[0, i].axis('off')\n\n# Row 2: Last method and ensemble results\naxes[1, 0].imshow(np_img)\naxes[1, 0].imshow(list(ensemble_explanations.values())[3], alpha=0.5, cmap='hot')\naxes[1, 0].set_title(list(ensemble_explanations.keys())[3].replace('_', ' ').title())\naxes[1, 0].axis('off')\n\naxes[1, 1].imshow(np_img)\naxes[1, 1].imshow(ensemble_avg, alpha=0.5, cmap='hot')\naxes[1, 1].set_title(\"Ensemble (Average)\")\naxes[1, 1].axis('off')\n\naxes[1, 2].imshow(np_img)\naxes[1, 2].imshow(ensemble_max, alpha=0.5, cmap='hot')\naxes[1, 2].set_title(\"Ensemble (Maximum)\")\naxes[1, 2].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 7. Advanced Visualization Techniques\n\nSignXAI2 provides various ways to visualize explanations. Let's explore different visualization options.\n\n# Get a high-quality explanation to visualize\nlrp_explanation = explain(\n    model=model,\n    x=preprocessed_img,\n    method_name='lrp_epsilon_0_1',\n    target_class=predicted_class\n)\nlrp_processed = preprocess_for_visualization(lrp_explanation)\n\n# 1. Visualization with different color maps and overlays\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Row 1: Different colormaps\ncolormaps = ['hot', 'jet', 'RdBu_r']\nfor i, cmap in enumerate(colormaps):\n    axes[0, i].imshow(np_img)\n    im = axes[0, i].imshow(lrp_processed, alpha=0.5, cmap=cmap)\n    axes[0, i].set_title(f\"{cmap} colormap\")\n    axes[0, i].axis('off')\n\n# Row 2: Different overlay techniques\n# Pure heatmap\naxes[1, 0].imshow(lrp_processed, cmap='hot')\naxes[1, 0].set_title(\"Pure Heatmap\")\naxes[1, 0].axis('off')\n\n# Masked overlay (only show high relevance areas)\nthreshold = 0.3\nmask = (lrp_processed.mean(axis=2) > threshold)\nmasked_img = np_img.copy()\nmasked_img[~mask] = masked_img[~mask] * 0.3  # Dim irrelevant areas\naxes[1, 1].imshow(masked_img)\naxes[1, 1].set_title(f\"Masked (threshold={threshold})\")\naxes[1, 1].axis('off')\n\n# Contour overlay\nfrom skimage import measure\ncontours = measure.find_contours(lrp_processed.mean(axis=2), 0.3)\naxes[1, 2].imshow(np_img)\nfor contour in contours:\n    axes[1, 2].plot(contour[:, 1], contour[:, 0], linewidth=2, color='red')\naxes[1, 2].set_title(\"Contour Overlay\")\naxes[1, 2].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2. Advanced: Multiple transparency levels and thresholding\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Different transparency levels\nalphas = [0.3, 0.5, 0.7]\nfor i, alpha in enumerate(alphas):\n    axes[0, i].imshow(np_img)\n    axes[0, i].imshow(lrp_processed, alpha=alpha, cmap='hot')\n    axes[0, i].set_title(f\"Alpha = {alpha}\")\n    axes[0, i].axis('off')\n\n# Different thresholding approaches\n# Top-k pixels\nk_percent = 20  # Show top 20% of pixels\nflat_values = lrp_processed.mean(axis=2).flatten()\nthreshold_k = np.percentile(flat_values, 100 - k_percent)\ntop_k_mask = lrp_processed.mean(axis=2) > threshold_k\n\naxes[1, 0].imshow(np_img)\naxes[1, 0].imshow(np.ma.masked_where(~top_k_mask, lrp_processed), alpha=0.7, cmap='hot')\naxes[1, 0].set_title(f\"Top {k_percent}% pixels\")\naxes[1, 0].axis('off')\n\n# Binary threshold\nbinary_threshold = 0.5\nbinary_mask = (lrp_processed.mean(axis=2) > binary_threshold).astype(float)\naxes[1, 1].imshow(np_img)\naxes[1, 1].imshow(binary_mask, alpha=0.5, cmap='Reds')\naxes[1, 1].set_title(f\"Binary (threshold={binary_threshold})\")\naxes[1, 1].axis('off')\n\n# Smooth gradient overlay\nsmooth_mask = lrp_processed.mean(axis=2)\naxes[1, 2].imshow(np_img)\naxes[1, 2].imshow(smooth_mask, alpha=0.6, cmap='plasma')\naxes[1, 2].set_title(\"Smooth Gradient\")\naxes[1, 2].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom LRP analyzer that tracks layer-wise relevance\n",
    "class LayerTrackingLRP(LRPZ):\n",
    "    def __init__(self, model, tracked_layers=None):\n",
    "        super().__init__(model)\n",
    "        self.tracked_layers = tracked_layers or []\n",
    "        self.layer_relevances = {}\n",
    "        \n",
    "        # Register hooks for tracking\n",
    "        self.hooks = []\n",
    "        for name, module in model.named_modules():\n",
    "            if any(tracked in name for tracked in self.tracked_layers):\n",
    "                hook = module.register_forward_hook(self._hook_fn(name))\n",
    "                self.hooks.append(hook)\n",
    "    \n",
    "    def _hook_fn(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.layer_relevances[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def __del__(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "# For VGG16, track feature layers at different depths\n",
    "tracked_layers = [\n",
    "    'features.1',    # Early layer (after first ReLU)\n",
    "    'features.15',   # Middle layer\n",
    "    'features.28',   # Deep layer (last feature layer)\n",
    "    'classifier.3'   # Final classifier layer\n",
    "]\n",
    "\n",
    "# Create the layer tracking analyzer\n",
    "tracking_lrp = LayerTrackingLRP(model_no_softmax, tracked_layers=tracked_layers)\n",
    "\n",
    "# Generate explanation\n",
    "tracking_explanation = tracking_lrp.attribute(preprocessed_img, target=predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Conclusion\n\nIn this advanced tutorial, we explored several sophisticated aspects of using SignXAI2 with PyTorch:\n\n1. **LRP Variants**: We demonstrated how to use different LRP methods (Z-rule, epsilon-rule, alpha-beta rules, and SIGN variants) through the unified API.\n\n2. **Advanced Parameters**: We showed how to customize explanation methods with various parameters like noise levels for SmoothGrad, step counts for Integrated Gradients, and layer selection for Grad-CAM.\n\n3. **Custom Models**: We demonstrated that SignXAI2's unified API works seamlessly with custom model architectures.\n\n4. **Ensemble Methods**: We showed how to combine multiple explanation methods to get more robust insights.\n\n5. **Advanced Visualization**: We explored different visualization techniques including various colormaps, transparency settings, masking, and thresholding approaches.\n\nThe unified API in SignXAI2 makes it easy to:\n- Switch between different explanation methods\n- Compare results across methods\n- Work with both pre-trained and custom models\n- Customize parameters for fine-tuned explanations\n- Use the same interface for both TensorFlow and PyTorch\n\nFor more information and to contribute to the project, visit the [SignXAI2 GitHub repository](https://github.com/IRISlaboratory/signxai2)."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}