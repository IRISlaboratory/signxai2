{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SignXAI2 with PyTorch - Basic Usage (VGG16)\n\nThis tutorial demonstrates how to use the SignXAI2 package with PyTorch to explain a VGG16 image classification model. We'll walk through:\n\n1. Setting up the environment\n2. Loading a pre-trained VGG16 model and sample image\n3. Generating explanations using the unified API\n4. Visualizing and comparing the results\n\n## Setup Requirements\n\n**Important**: SignXAI2 requires Python 3.9 or 3.10 (Python 3.11+ is not supported)\n\nSince you're running this tutorial, you should already have cloned the signxai2 repository. From the repository root directory:\n\n### Using conda:\n```bash\n# Create environment with Python 3.10\nconda create -n signxai2 python=3.10\nconda activate signxai2\n\n# Install PyTorch dependencies only\npip install -r requirements/common.txt -r requirements/pytorch.txt\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\n### Using venv:\n```bash\n# Create virtual environment\npython3.10 -m venv signxai2_env\nsource signxai2_env/bin/activate  # On Windows: signxai2_env\\Scripts\\activate\n\n# Install PyTorch dependencies only\npip install -r requirements/common.txt -r requirements/pytorch.txt\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\nLet's get started!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\n# SignXAI2 unified API imports\nfrom signxai import explain, list_methods"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up data and model paths\n_THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\nDATA_DIR = os.path.realpath(os.path.join(_THIS_DIR, \"..\", \"..\", \"data\"))\nVGG16_MODEL_PATH = os.path.join(DATA_DIR, \"models\", \"pytorch\", \"VGG16\", \"vgg16_ported_weights.pt\")\nIMAGE_PATH = os.path.join(DATA_DIR, \"images\", \"example.jpg\")\n\n# Check that files exist\nassert os.path.exists(VGG16_MODEL_PATH), f\"VGG16 model not found at {VGG16_MODEL_PATH}\"\nassert os.path.exists(IMAGE_PATH), f\"Image not found at {IMAGE_PATH}\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load VGG16 Model and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the VGG16 model definition\nsys.path.append(os.path.join(DATA_DIR, \"models\", \"pytorch\", \"VGG16\"))\nfrom VGG16 import VGG16_PyTorch\n\n# Load the VGG16 model\ntry:\n    # Initialize the model architecture\n    vgg16_model = VGG16_PyTorch(num_classes=1000)\n    # Load the pre-trained weights\n    vgg16_model.load_state_dict(torch.load(VGG16_MODEL_PATH, map_location=torch.device('cpu')))\n    print(\"Loaded pre-saved VGG16 model weights\")\nexcept Exception as e:\n    # If that fails, load the model from torchvision\n    print(f\"Could not load saved model: {e}\\nLoading from torchvision instead\")\n    import torchvision.models as models\n    vgg16_model = models.vgg16(pretrained=True)\n    \nvgg16_model.eval()  # Set to evaluation mode\nprint(f\"Model loaded successfully. Type: {type(vgg16_model).__name__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess image for VGG16\n",
    "def load_image(image_path, resize_dim=(224, 224)):\n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Display original image\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Original Image')\n",
    "    plt.show()\n",
    "    \n",
    "    # Define preprocessing for VGG16\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(resize_dim),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Preprocess image\n",
    "    input_tensor = preprocess(img)\n",
    "    \n",
    "    # Create a mini-batch as expected by the model\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    \n",
    "    return img, input_batch\n",
    "\n",
    "# Load and preprocess the image\n",
    "original_img, preprocessed_img = load_image(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict the Class with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with VGG16\n",
    "with torch.no_grad():\n",
    "    output = vgg16_model(preprocessed_img)\n",
    "\n",
    "# Get the predicted class index\n",
    "_, predicted_idx = torch.max(output, 1)\n",
    "predicted_class = predicted_idx.item()\n",
    "\n",
    "# Load class labels (ImageNet classes for VGG16)\n",
    "try:\n",
    "    import json\n",
    "    with open(os.path.join(DATA_DIR, \"imagenet_class_index.json\")) as f:\n",
    "        class_idx = json.load(f)\n",
    "    class_name = class_idx[str(predicted_class)][1]\n",
    "except:\n",
    "    # If class file doesn't exist, just use the class index\n",
    "    class_name = f\"Class {predicted_class}\"\n",
    "\n",
    "print(f\"Predicted class: {class_name} (index: {predicted_class})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Make a prediction with VGG16\nwith torch.no_grad():\n    output = vgg16_model(preprocessed_img)\n\n# Get the predicted class index\n_, predicted_idx = torch.max(output, 1)\npredicted_class = predicted_idx.item()\n\n# Load class labels (ImageNet classes for VGG16)\ntry:\n    import json\n    imagenet_class_path = os.path.join(DATA_DIR, \"imagenet_class_index.json\")\n    if os.path.exists(imagenet_class_path):\n        with open(imagenet_class_path) as f:\n            class_idx = json.load(f)\n        class_name = class_idx[str(predicted_class)][1]\n    else:\n        # Try to decode using torchvision if available\n        from torchvision.models import VGG16_Weights\n        weights = VGG16_Weights.DEFAULT\n        class_name = weights.meta[\"categories\"][predicted_class]\nexcept:\n    # If class file doesn't exist, just use the class index\n    class_name = f\"Class {predicted_class}\"\n\nprint(f\"Predicted class: {class_name} (index: {predicted_class})\")"
  },
  {
   "cell_type": "code",
   "source": "# List all available methods in SignXAI2\nprint(\"Available methods in SignXAI2:\")\navailable_methods = list_methods()\nprint(f\"Total methods: {len(available_methods)}\")\nprint(\"\\nSome common methods:\")\nfor method in ['gradient', 'gradient_x_input', 'grad_cam', 'guided_backprop', 'lrp_z']:\n    if method in available_methods:\n        print(f\"  - {method}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Generate Explanations with SignXAI2\n\nNow let's use SignXAI2's unified API to explain the VGG16 model's prediction using different methods."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the methods we want to use for VGG16 explanation\n# Using the unified API method names\nmethods_to_test = [\n    'gradient',\n    'gradient_x_input',\n    'grad_cam',\n    'guided_backprop', \n    'lrp_z',\n    'lrp_epsilon_0_1'\n]\n\n# Find the target layer name for GradCAM (last convolutional layer in VGG16)\n# For PyTorch VGG16, it's typically the last conv layer in features\ntarget_layer_name = None\nfor name, module in vgg16_model.features.named_children():\n    if isinstance(module, nn.Conv2d):\n        target_layer_name = f\"features.{name}\"\n\nprint(f\"Target layer for GradCAM: {target_layer_name}\")\n\n# Additional parameters for specific methods\nmethod_params = {\n    'grad_cam': {'target_layer': target_layer_name}  # PyTorch uses 'target_layer' instead of 'layer_name'\n}\n\n# Storage for explanations\nexplanations = {}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Generate explanations using the unified API\nfor method_name in methods_to_test:\n    print(f\"Generating {method_name} explanation...\")\n    \n    # Get method-specific parameters\n    params = method_params.get(method_name, {})\n    \n    # Use the unified explain API\n    explanation = explain(\n        model=vgg16_model,\n        x=preprocessed_img,\n        method_name=method_name,\n        target_class=predicted_class,\n        **params\n    )\n    \n    explanations[method_name] = explanation\n    \nprint(\"All explanations generated!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for prettier visualization\n",
    "def preprocess_explanation(explanation):\n",
    "    # Handle PyTorch tensor shape [B, C, H, W]\n",
    "    if explanation.ndim == 4:\n",
    "        explanation = np.transpose(explanation[0], (1, 2, 0))  # [H, W, C]\n",
    "    elif explanation.ndim == 3 and explanation.shape[0] == 3:  # If it's [C, H, W]\n",
    "        explanation = np.transpose(explanation, (1, 2, 0))      # [H, W, C]\n",
    "    \n",
    "    # Use absolute values for gradient-based methods\n",
    "    abs_explanation = np.abs(explanation)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    if abs_explanation.max() > 0:\n",
    "        abs_explanation = abs_explanation / abs_explanation.max()\n",
    "    \n",
    "    return abs_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Utility function for prettier visualization\ndef preprocess_explanation(explanation):\n    # Convert to numpy if it's a torch tensor\n    if torch.is_tensor(explanation):\n        explanation = explanation.detach().cpu().numpy()\n    \n    # Handle PyTorch tensor shape [B, C, H, W]\n    if explanation.ndim == 4:\n        explanation = np.transpose(explanation[0], (1, 2, 0))  # [H, W, C]\n    elif explanation.ndim == 3 and explanation.shape[0] == 3:  # If it's [C, H, W]\n        explanation = np.transpose(explanation, (1, 2, 0))      # [H, W, C]\n    \n    # Convert single channel to RGB\n    if explanation.ndim == 2:\n        explanation = np.expand_dims(explanation, axis=-1)\n        explanation = np.repeat(explanation, 3, axis=-1)\n    elif explanation.shape[-1] == 1:\n        explanation = np.repeat(explanation, 3, axis=-1)\n    \n    # Use absolute values for gradient-based methods\n    abs_explanation = np.abs(explanation)\n    \n    # Normalize for visualization\n    if abs_explanation.max() > 0:\n        abs_explanation = abs_explanation / abs_explanation.max()\n    \n    return abs_explanation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Convert PIL image to numpy for visualization\nnp_img = np.array(original_img.resize((224, 224)))\n\n# Create figure for visualization\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, (method_name, explanation) in enumerate(explanations.items()):\n    processed_explanation = preprocess_explanation(explanation)\n    \n    # Simple overlay visualization\n    axes[i].imshow(np_img)\n    axes[i].imshow(processed_explanation, alpha=0.5, cmap='hot')\n    axes[i].set_title(method_name.replace('_', ' ').title())\n    axes[i].axis('off')\n\nplt.suptitle(f\"VGG16 Explanations for class: {class_name}\", fontsize=16)\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)  # Adjust for the suptitle\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TensorFlow-style API\n",
    "from signxai.torch_signxai import tf_calculate_relevancemap\n",
    "\n",
    "# Use the TensorFlow-style API with PyTorch model\n",
    "methods_tf_style = ['gradient', 'gradient_x_input', 'guided_backprop', 'lrp_z', 'lrp_epsilon_0_1']\n",
    "explanations_tf_style = {}\n",
    "\n",
    "for method in methods_tf_style:\n",
    "    print(f\"Generating explanation using TF-style API: {method}...\")\n",
    "    \n",
    "    # Using the TensorFlow-style API with PyTorch model\n",
    "    explanation = tf_calculate_relevancemap(method, preprocessed_img, vgg16_model_no_softmax)\n",
    "    \n",
    "    # Store explanation\n",
    "    explanations_tf_style[method] = explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Conclusion\n\nIn this tutorial, we've learned how to:\n- Set up SignXAI2 with PyTorch\n- Load and prepare a pre-trained VGG16 model for explanation\n- Apply various explainability methods using the unified API\n- Visualize and interpret the results\n\nThe SignXAI2 unified API makes it easy to:\n- Use the same interface for both TensorFlow and PyTorch models\n- Switch between different explanation methods effortlessly\n- Apply framework-specific optimizations automatically\n\nVGG16 is an excellent model for explainability demonstrations because of its straightforward architecture. The clear convolutional structure makes it easier to interpret how different parts of the image influence the model's predictions.\n\nFor more advanced techniques and detailed explanations of other models, check out the other tutorials in this series."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}