{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SignXAI with PyTorch - Basic Usage (VGG16)\n\nThis tutorial demonstrates how to use the SignXAI package with PyTorch to explain a VGG16 image classification model. We'll walk through:\n\n1. Setting up the environment\n2. Loading a pre-trained VGG16 model and sample image\n3. Generating explanations using different methods\n4. Visualizing and comparing the results\n\n## Setup Requirements\n\nFor this PyTorch tutorial, you'll need to install SignXAI with PyTorch dependencies:\n\n```bash\n# For conda users\nconda create -n signxai-pytorch python=3.8\nconda activate signxai-pytorch\npip install -r ../../requirements/common.txt\npip install -r ../../requirements/pytorch.txt\n\n# Or for pip users\npython -m venv signxai_pytorch_env\nsource signxai_pytorch_env/bin/activate  # On Windows: signxai_pytorch_env\\Scripts\\activate\npip install -r ../../requirements/common.txt\npip install -r ../../requirements/pytorch.txt\n```\n\nLet's get started!",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# SignXAI imports\n",
    "from signxai.torch_signxai.methods import SIGN, GradCAM, GuidedBackprop, LRPZ, LRPEpsilon\n",
    "from signxai.common.visualization import visualize_attribution\n",
    "from signxai.torch_signxai.utils import remove_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and model paths\n",
    "_THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "DATA_DIR = os.path.realpath(os.path.join(_THIS_DIR, \"..\", \"..\", \"data\"))\n",
    "VGG16_MODEL_PATH = os.path.join(DATA_DIR, \"models\", \"pytorch\", \"VGG16\", \"vgg16_torch.pt\")\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, \"images\", \"example.jpg\")\n",
    "\n",
    "# Check that files exist\n",
    "assert os.path.exists(VGG16_MODEL_PATH), f\"VGG16 model not found at {VGG16_MODEL_PATH}\"\n",
    "assert os.path.exists(IMAGE_PATH), f\"Image not found at {IMAGE_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load VGG16 Model and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model\n",
    "try:\n",
    "    # First try loading our pre-saved model\n",
    "    vgg16_model = torch.load(VGG16_MODEL_PATH)\n",
    "    print(\"Loaded pre-saved VGG16 model\")\n",
    "except Exception as e:\n",
    "    # If that fails, load the model from torchvision\n",
    "    print(f\"Could not load saved model: {e}\\nLoading from torchvision instead\")\n",
    "    import torchvision.models as models\n",
    "    vgg16_model = models.vgg16(pretrained=True)\n",
    "    \n",
    "vgg16_model.eval()  # Set to evaluation mode\n",
    "print(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove softmax from the VGG16 model - crucial step for many explainability methods\n",
    "vgg16_model_no_softmax = remove_softmax(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess image for VGG16\n",
    "def load_image(image_path, resize_dim=(224, 224)):\n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Display original image\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Original Image')\n",
    "    plt.show()\n",
    "    \n",
    "    # Define preprocessing for VGG16\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(resize_dim),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Preprocess image\n",
    "    input_tensor = preprocess(img)\n",
    "    \n",
    "    # Create a mini-batch as expected by the model\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    \n",
    "    return img, input_batch\n",
    "\n",
    "# Load and preprocess the image\n",
    "original_img, preprocessed_img = load_image(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict the Class with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with VGG16\n",
    "with torch.no_grad():\n",
    "    output = vgg16_model(preprocessed_img)\n",
    "\n",
    "# Get the predicted class index\n",
    "_, predicted_idx = torch.max(output, 1)\n",
    "predicted_class = predicted_idx.item()\n",
    "\n",
    "# Load class labels (ImageNet classes for VGG16)\n",
    "try:\n",
    "    import json\n",
    "    with open(os.path.join(DATA_DIR, \"imagenet_class_index.json\")) as f:\n",
    "        class_idx = json.load(f)\n",
    "    class_name = class_idx[str(predicted_class)][1]\n",
    "except:\n",
    "    # If class file doesn't exist, just use the class index\n",
    "    class_name = f\"Class {predicted_class}\"\n",
    "\n",
    "print(f\"Predicted class: {class_name} (index: {predicted_class})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Explanations with SignXAI\n",
    "\n",
    "Now let's use SignXAI to explain the VGG16 model's prediction using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the methods we want to use for VGG16 explanation\n",
    "# Find the target layer for GradCAM (last convolutional layer in VGG16)\n",
    "target_layer = None\n",
    "for name, module in vgg16_model.features.named_children():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        target_layer = module\n",
    "\n",
    "# Initialize explainers\n",
    "methods = {\n",
    "    'Gradient': SIGN(vgg16_model_no_softmax),\n",
    "    'Gradient × Input': SIGN(vgg16_model_no_softmax),  # We'll multiply by input later\n",
    "    'GradCAM': GradCAM(vgg16_model_no_softmax, target_layer),\n",
    "    'Guided Backprop': GuidedBackprop(vgg16_model_no_softmax),\n",
    "    'LRP-Z': LRPZ(vgg16_model_no_softmax),\n",
    "    'LRP-Epsilon': LRPEpsilon(vgg16_model_no_softmax, epsilon=0.1)\n",
    "}\n",
    "\n",
    "# Storage for explanations\n",
    "explanations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanations for VGG16\n",
    "for method_name, explainer in methods.items():\n",
    "    print(f\"Generating {method_name} explanation...\")\n",
    "    \n",
    "    if method_name == 'Gradient × Input':\n",
    "        # Special case for gradient × input\n",
    "        grad = explainer.attribute(preprocessed_img, target=predicted_class).numpy()\n",
    "        explanation = grad * preprocessed_img.numpy()\n",
    "    else:\n",
    "        explanation = explainer.attribute(preprocessed_img, target=predicted_class).numpy()\n",
    "    \n",
    "    explanations[method_name] = explanation\n",
    "    \n",
    "print(\"All explanations generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize VGG16 Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for prettier visualization\n",
    "def preprocess_explanation(explanation):\n",
    "    # Handle PyTorch tensor shape [B, C, H, W]\n",
    "    if explanation.ndim == 4:\n",
    "        explanation = np.transpose(explanation[0], (1, 2, 0))  # [H, W, C]\n",
    "    elif explanation.ndim == 3 and explanation.shape[0] == 3:  # If it's [C, H, W]\n",
    "        explanation = np.transpose(explanation, (1, 2, 0))      # [H, W, C]\n",
    "    \n",
    "    # Use absolute values for gradient-based methods\n",
    "    abs_explanation = np.abs(explanation)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    if abs_explanation.max() > 0:\n",
    "        abs_explanation = abs_explanation / abs_explanation.max()\n",
    "    \n",
    "    return abs_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PIL image to numpy for visualization\n",
    "np_img = np.array(original_img.resize((224, 224)))\n",
    "\n",
    "# Create figure for visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (method_name, explanation) in enumerate(explanations.items()):\n",
    "    processed_explanation = preprocess_explanation(explanation)\n",
    "    \n",
    "    # Use SignXAI's visualization utility\n",
    "    visualize_attribution(image=np_img, attribution=processed_explanation, ax=axes[i])\n",
    "    axes[i].set_title(method_name)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"VGG16 Explanations for class: {class_name}\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)  # Adjust for the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpret the VGG16 Results\n",
    "\n",
    "Let's interpret what we're seeing in these explanation methods for the VGG16 model:\n",
    "\n",
    "- **Gradient**: Shows pixel-level importance via the gradient of the output with respect to input. In VGG16, this often highlights edges and textures.\n",
    "\n",
    "- **Gradient × Input**: Enhances gradient by multiplication with input values. This tends to focus more on the regions where both the gradient and input values are high.\n",
    "\n",
    "- **GradCAM**: Uses the last convolutional layer of VGG16 to produce a coarse localization map highlighting important regions for the prediction.\n",
    "\n",
    "- **Guided Backprop**: Creates sharper feature visualizations by modifying the backpropagation signal through ReLU layers. It's particularly effective for VGG16 which has many ReLU activations.\n",
    "\n",
    "- **LRP-Z**: Layer-wise Relevance Propagation with the Z-rule propagates the prediction backward through the network to identify relevant input features.\n",
    "\n",
    "- **LRP-Epsilon**: A variant of LRP that adds a stabilizing term (epsilon) to avoid division by zero, producing slightly different attribution maps.\n",
    "\n",
    "Each method highlights different aspects of how VGG16 processes the image to make its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using SignXAI's TensorFlow API with PyTorch\n",
    "\n",
    "One of the advantages of SignXAI is its compatibility API, which allows you to use the TensorFlow-style API even with PyTorch models. This is useful if you're migrating from TensorFlow to PyTorch or if you're more familiar with the TensorFlow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TensorFlow-style API\n",
    "from signxai.torch_signxai import tf_calculate_relevancemap\n",
    "\n",
    "# Use the TensorFlow-style API with PyTorch model\n",
    "methods_tf_style = ['gradient', 'gradient_x_input', 'guided_backprop', 'lrp_z', 'lrp_epsilon_0_1']\n",
    "explanations_tf_style = {}\n",
    "\n",
    "for method in methods_tf_style:\n",
    "    print(f\"Generating explanation using TF-style API: {method}...\")\n",
    "    \n",
    "    # Using the TensorFlow-style API with PyTorch model\n",
    "    explanation = tf_calculate_relevancemap(method, preprocessed_img, vgg16_model_no_softmax)\n",
    "    \n",
    "    # Store explanation\n",
    "    explanations_tf_style[method] = explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explanations generated with TensorFlow-style API\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (method_name, explanation) in enumerate(explanations_tf_style.items()):\n",
    "    if i >= len(axes):\n",
    "        break\n",
    "        \n",
    "    processed_explanation = preprocess_explanation(explanation)\n",
    "    \n",
    "    # Use SignXAI's visualization utility\n",
    "    visualize_attribution(image=np_img, attribution=processed_explanation, ax=axes[i])\n",
    "    axes[i].set_title(f\"TF-API: {method_name}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Hide any unused axes\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle(f\"TensorFlow-style API with PyTorch VGG16\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)  # Adjust for the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this tutorial, we've learned how to:\n",
    "- Set up SignXAI with PyTorch\n",
    "- Load and prepare a pre-trained VGG16 model for explanation\n",
    "- Apply various explainability methods to understand VGG16 predictions\n",
    "- Visualize and interpret the results\n",
    "- Use SignXAI's TensorFlow-style API with PyTorch models\n",
    "\n",
    "VGG16 is an excellent model for explainability demonstrations because of its straightforward architecture. The clear convolutional structure makes it easier to interpret how different parts of the image influence the model's predictions.\n",
    "\n",
    "For more advanced techniques and detailed explanations of other models, check out the other tutorials in this series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}