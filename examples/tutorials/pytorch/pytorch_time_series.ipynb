{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SignXAI2 with PyTorch - ECG Time Series Explanation\n\nThis tutorial demonstrates how to use SignXAI2 to explain ECG (electrocardiogram) predictions with PyTorch models. We'll be working with ECG data and a PyTorch model for detecting cardiac conditions.\n\n## Setup Requirements\n\n**Important**: SignXAI2 requires Python 3.9 or 3.10 (Python 3.11+ is not supported)\n\nSince you're running this tutorial, you should already have cloned the signxai2 repository. From the repository root directory:\n\n### Using conda:\n```bash\n# Create environment with Python 3.10\nconda create -n signxai2 python=3.10\nconda activate signxai2\n\n# Install PyTorch dependencies only\npip install -r requirements/common.txt -r requirements/pytorch.txt\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\n### Using venv:\n```bash\n# Create virtual environment\npython3.10 -m venv signxai2_env\nsource signxai2_env/bin/activate  # On Windows: signxai2_env\\Scripts\\activate\n\n# Install PyTorch dependencies only\npip install -r requirements/common.txt -r requirements/pytorch.txt\n\n# Download models and example data\ngit lfs install\nbash ./prepare.sh\n```\n\n## Overview\n\nWe'll cover:\n1. Loading ECG data and a PyTorch model\n2. Pre-processing ECG signals\n3. Applying explainability methods to time series data using the unified API\n4. Visualizing the regions of the ECG that influenced model predictions\n\nLet's get started!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# SignXAI2 unified API imports\nfrom signxai import explain, list_methods\n\n# Import utilities for ECG data handling\n# Ensure the utils directory is in the Python path\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"..\", \"utils\"))\ntry:\n    from ecg.explainability import normalize_ecg_relevancemap\nexcept ImportError:\n    # Define a simple normalization function if the utility is not available\n    def normalize_ecg_relevancemap(relevancemap):\n        \"\"\"Normalize relevance map to [0, 1] range.\"\"\"\n        if relevancemap.max() > relevancemap.min():\n            return (relevancemap - relevancemap.min()) / (relevancemap.max() - relevancemap.min())\n        return relevancemap"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up paths\n_THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\nDATA_DIR = os.path.realpath(os.path.join(_THIS_DIR, \"..\", \"..\", \"data\"))\nTIMESERIES_DIR = os.path.join(DATA_DIR, \"timeseries\")\nECG_MODEL_PATH = os.path.join(DATA_DIR, \"models\", \"pytorch\", \"ECG\", \"ecg_ported_weights.pt\")\n\n# Check that model file exists\nif not os.path.exists(ECG_MODEL_PATH):\n    print(f\"ECG model not found at {ECG_MODEL_PATH}\")\n    print(\"We'll create a demo model for this tutorial\")\nelse:\n    print(f\"Found ECG model at {ECG_MODEL_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a PyTorch ECG Model\n",
    "\n",
    "Let's first define a PyTorch model architecture for ECG processing. This is a 1D convolutional neural network designed for time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ECGModel(nn.Module):\n    \"\"\"Demo ECG model for tutorial - matches the structure of ECG_PyTorch\"\"\"\n    def __init__(self, input_channels=1, num_classes=1):\n        super(ECGModel, self).__init__()\n        self.features = nn.Sequential(\n            # First convolutional block\n            nn.Conv1d(input_channels, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2, stride=2),  # 1000\n            \n            # Second convolutional block\n            nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=4, stride=4),  # 250\n            \n            # Third convolutional block\n            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2, stride=2),  # 125\n            \n            # Fourth convolutional block\n            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=5, stride=5),  # 25\n        )\n        \n        # Flatten and fully connected layers\n        self.fc = nn.Sequential(\n            nn.Linear(256 * 25, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes),  # Binary classification\n            nn.Sigmoid()  # Use sigmoid for binary classification\n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc(x)\n        return x\n    \n    # Convenience method to get the last convolutional layer (for GradCAM)\n    def get_last_conv_layer(self):\n        return self.features[9]  # The 4th Conv1d layer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and ECG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the actual ECG model if available\nsys.path.append(os.path.join(DATA_DIR, \"models\", \"pytorch\", \"ECG\"))\n\n# Load the PyTorch ECG model\ntry:\n    from ecg_model import ECG_PyTorch\n    # Initialize the model architecture\n    ecg_model = ECG_PyTorch(input_channels=1, num_classes=3)\n    # Load the pre-trained weights\n    ecg_model.load_state_dict(torch.load(ECG_MODEL_PATH, map_location=torch.device('cpu')))\n    print(\"Loaded pre-trained ECG model weights\")\nexcept Exception as e:\n    print(f\"Could not load saved model: {e}\\nInitializing demo model instead\")\n    # Use the demo model defined above\n    ecg_model = ECGModel()\n\n# Set model to evaluation mode\necg_model.eval()\n\n# Print model architecture\nprint(f\"Model type: {type(ecg_model).__name__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample ECG\n",
    "try:\n",
    "    # Try to load from saved numpy file\n",
    "    ecg_sample_path = os.path.join(TIMESERIES_DIR, \"ecg_sample.npy\")\n",
    "    ecg_data = np.load(ecg_sample_path)\n",
    "    print(f\"Loaded ECG sample from {ecg_sample_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load ECG sample: {e}\\nGenerating synthetic data instead\")\n",
    "    # Generate synthetic ECG data if loading fails\n",
    "    # This is simplified and not realistic, just for illustration\n",
    "    from scipy import signal\n",
    "    t = np.linspace(0, 10, 2000)\n",
    "    ecg_data = signal.square(2 * np.pi * 1.2 * t, duty=0.3) + np.sin(2 * np.pi * 3 * t)\n",
    "    ecg_data += np.random.normal(0, 0.1, size=ecg_data.shape)\n",
    "\n",
    "# Plot the ECG data\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(ecg_data)\n",
    "plt.title(\"ECG Sample\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Preprocess for PyTorch model (add channel dimension and batch dimension)\n",
    "ecg_tensor = torch.tensor(ecg_data, dtype=torch.float32).view(1, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model prediction\n",
    "with torch.no_grad():\n",
    "    prediction = ecg_model(ecg_tensor)\n",
    "    prediction_probability = prediction.item()\n",
    "\n",
    "print(f\"Model prediction: {prediction_probability:.4f}\")\n",
    "print(f\"Diagnosis: {'Positive' if prediction_probability > 0.5 else 'Negative'} for abnormal ECG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Generate Explanations for the ECG Signal Using the Unified API"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the explainability methods to use with the unified API\nmethods_to_test = [\n    'gradient',\n    'gradient_x_input',\n    'grad_cam',\n    'guided_backprop',\n    'lrp_z',\n    'lrp_epsilon_0_5'\n]\n\n# Method-specific parameters\nmethod_params = {\n    'grad_cam': {'target_layer': 'features.9'}  # Last conv layer in our ECG model\n}\n\n# Storage for explanations\nexplanations = {}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate explanations using the unified API\n# For binary classification, we use the predicted class as target\ntarget_class = 0 if prediction_probability < 0.5 else 1\n\nfor method_name in methods_to_test:\n    print(f\"Generating {method_name} explanation...\")\n    \n    # Get method-specific parameters\n    params = method_params.get(method_name, {})\n    \n    # Use the unified explain API\n    explanation = explain(\n        model=ecg_model,\n        x=ecg_tensor,\n        method_name=method_name,\n        target_class=target_class,\n        **params\n    )\n    \n    # Convert to numpy if it's a tensor\n    if torch.is_tensor(explanation):\n        explanation = explanation.detach().cpu().numpy()\n    \n    # Only use positive values for visualization\n    explanation = np.maximum(explanation, 0)\n    \n    # Normalize the explanation for visualization\n    explanation = normalize_ecg_relevancemap(explanation)\n    \n    # Add to explanations dictionary\n    explanations[method_name] = explanation\n    \nprint(\"All explanations generated!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize ECG Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert explanations to proper shape for visualization\n",
    "def reshape_for_visualization(explanation):\n",
    "    # For 1D time series data, extract the values and flatten\n",
    "    if explanation.ndim == 3:  # [batch, channel, time]\n",
    "        return explanation[0, 0, :]\n",
    "    elif explanation.ndim == 4:  # [batch, channel, height, width]\n",
    "        # For GradCAM, the result might need to be resized\n",
    "        from scipy.interpolate import interp1d\n",
    "        orig_size = ecg_data.shape[0]\n",
    "        expl = explanation[0, 0]\n",
    "        if expl.shape[0] != orig_size:\n",
    "            x_orig = np.linspace(0, 1, expl.shape[0])\n",
    "            x_new = np.linspace(0, 1, orig_size)\n",
    "            f = interp1d(x_orig, expl, kind='linear')\n",
    "            return f(x_new)\n",
    "        return expl\n",
    "    return explanation\n",
    "\n",
    "# Process explanations\n",
    "processed_explanations = {}\n",
    "for method_name, explanation in explanations.items():\n",
    "    processed_explanations[method_name] = reshape_for_visualization(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter explanations for better visualization (optional)\n",
    "posthresh = 0.2  # Threshold for filtering low values\n",
    "cmap_adjust = 0.3  # Adjust colormap for better visibility\n",
    "\n",
    "for method_name, explanation in processed_explanations.items():\n",
    "    # Filter low values\n",
    "    filtered_explanation = explanation.copy()\n",
    "    filtered_explanation[filtered_explanation <= posthresh] = 0\n",
    "    filtered_explanation[filtered_explanation > posthresh] += cmap_adjust\n",
    "    \n",
    "    # Update the explanations\n",
    "    processed_explanations[method_name] = filtered_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a figure to visualize all explanations\nfig, axes = plt.subplots(len(processed_explanations), 1, figsize=(15, 4*len(processed_explanations)))\n\n# Ensure axes is always a list\nif len(processed_explanations) == 1:\n    axes = [axes]\n\nfor i, (method_name, explanation) in enumerate(processed_explanations.items()):\n    ax = axes[i]\n    \n    # Plot the ECG\n    ax.plot(ecg_data, color='blue', alpha=0.7)\n    \n    # Create a colormap for the explanation\n    ax_twin = ax.twinx()\n    ax_twin.fill_between(range(len(ecg_data)), 0, explanation, \n                         color='red', alpha=0.5, label='Explanation')\n    ax_twin.set_ylim(0, 1.1)\n    ax_twin.set_ylabel('Relevance', color='red')\n    ax_twin.tick_params(axis='y', labelcolor='red')\n    \n    # Set title and labels\n    ax.set_title(f\"{method_name.replace('_', ' ').upper()} Explanation\")\n    ax.set_xlabel('Time (samples)')\n    ax.set_ylabel('ECG Amplitude', color='blue')\n    ax.tick_params(axis='y', labelcolor='blue')\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Interpretation of ECG Explanations\n\nLet's interpret what we're seeing in the ECG explanations:\n\n- **gradient**: Shows the sensitivity of the model output to changes in the input ECG signal. Peaks often correspond to key cardiac events (P-waves, QRS complexes, T-waves).\n\n- **gradient_x_input**: Enhances the gradient by multiplying it with the input signal, emphasizing areas where both the signal and its importance are high.\n\n- **grad_cam**: Adapts the Grad-CAM technique for 1D time series data, highlighting temporal regions that contributed most to the classification decision.\n\n- **guided_backprop**: Creates sharper feature visualizations by modifying the backpropagation signal through ReLU layers.\n\n- **lrp_z**: Layer-wise Relevance Propagation with the Z-rule propagates the prediction backward through the network to identify relevant input features.\n\n- **lrp_epsilon_0_5**: A variant of LRP that adds a stabilizing term (epsilon=0.5) to avoid division by zero, producing slightly different attribution maps.\n\nThe highlighted regions in the ECG indicate which parts of the signal were most important for the model's prediction. In cardiology, these regions often correspond to specific cardiac events like abnormal QRS complexes or ST-segment abnormalities."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TensorFlow-style API\n",
    "from signxai.torch_signxai import tf_calculate_relevancemap\n",
    "\n",
    "# Use the TensorFlow-style API with PyTorch model\n",
    "methods_tf_style = ['gradient', 'gradient_x_input', 'guided_backprop', 'lrp_z', 'lrp_epsilon_0_1']\n",
    "explanations_tf_style = {}\n",
    "\n",
    "for method in methods_tf_style:\n",
    "    print(f\"Generating explanation using TF-style API: {method}...\")\n",
    "    \n",
    "    # Using the TensorFlow-style API with PyTorch model\n",
    "    explanation = tf_calculate_relevancemap(method, ecg_tensor, ecg_model_no_softmax)\n",
    "    \n",
    "    # Process for visualization\n",
    "    explanation = np.maximum(explanation, 0)  # Only use positive values\n",
    "    explanation = normalize_ecg_relevancemap(explanation)\n",
    "    explanation = reshape_for_visualization(explanation)\n",
    "    \n",
    "    # Filter low values\n",
    "    explanation[explanation <= posthresh] = 0\n",
    "    explanation[explanation > posthresh] += cmap_adjust\n",
    "    \n",
    "    # Store explanation\n",
    "    explanations_tf_style[method] = explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Conclusion\n\nIn this tutorial, we've demonstrated how to use SignXAI2's unified API to explain predictions of ECG models in PyTorch. We've covered:\n\n- Creating a PyTorch model for ECG classification\n- Loading and preprocessing ECG time series data\n- Applying various explainability methods to ECG signals using the unified `explain()` function\n- Visualizing and interpreting explanations\n\nKey advantages of using SignXAI2's unified API:\n- Consistent interface across all explanation methods\n- Easy switching between different XAI techniques\n- Automatic parameter handling for time series data\n- Framework-agnostic approach (same API for TensorFlow and PyTorch)\n\nThese techniques help medical professionals understand what parts of the ECG signal are contributing to the model's predictions, potentially enhancing trust and enabling model validation for clinical use.\n\nThe ability to use the same API for both PyTorch and TensorFlow models makes SignXAI2 a valuable tool for researchers and practitioners working with either framework."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}